#!/usr/bin/env python3
"""
AI PM Framework - Workerå‡¦ç†è¦ªã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã‚¿ã‚¹ã‚¯èª­ã¿è¾¼ã¿ â†’ claude -p ã§ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ â†’ REPORTä½œæˆ â†’ DBæ›´æ–° â†’ ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚­ãƒ¥ãƒ¼è¿½åŠ 
ã‚’1ã‚³ãƒžãƒ³ãƒ‰ã§å®Œçµã•ã›ã‚‹ã€‚

Usage:
    python backend/worker/execute_task.py PROJECT_NAME TASK_ID [options]

Options:
    --dry-run       å®Ÿè¡Œè¨ˆç”»ã®ã¿è¡¨ç¤ºï¼ˆAIå‘¼ã³å‡ºã—ãƒ»DBæ›´æ–°ãªã—ï¼‰
    --skip-ai       AIå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ã®ã¿ï¼‰
    --verbose       è©³ç´°ãƒ­ã‚°å‡ºåŠ›
    --json          JSONå½¢å¼ã§å‡ºåŠ›
    --timeout SEC   claude -p ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 600ï¼‰
    --model MODEL   AIãƒ¢ãƒ‡ãƒ«ï¼ˆhaiku/sonnet/opusã€ã‚¿ã‚¹ã‚¯æŽ¨å¥¨ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°ãã¡ã‚‰ã‚’å„ªå…ˆï¼‰
    --auto-review   Workerå®Œäº†å¾Œã«ãƒ¬ãƒ“ãƒ¥ãƒ¼å‡¦ç†ã‚’è‡ªå‹•å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ‰åŠ¹ï¼‰
    --no-review     è‡ªå‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç„¡åŠ¹åŒ–ï¼ˆæ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹å ´åˆï¼‰
    --review-model MODEL  ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨AIãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: sonnetï¼‰
    --loop          ã‚¿ã‚¹ã‚¯å®Œäº†å¾Œã«æ¬¡ã®QUEUEDã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•èµ·å‹•ï¼ˆé€£ç¶šå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ï¼‰
    --max-tasks N   é€£ç¶šå®Ÿè¡Œæ™‚ã®æœ€å¤§ã‚¿ã‚¹ã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰

Example:
    python backend/worker/execute_task.py AI_PM_PJ TASK_602
    python backend/worker/execute_task.py AI_PM_PJ TASK_602 --dry-run
    python backend/worker/execute_task.py AI_PM_PJ TASK_602 --model opus
    python backend/worker/execute_task.py AI_PM_PJ TASK_602 --auto-review
    python backend/worker/execute_task.py AI_PM_PJ TASK_602 --loop  # é€£ç¶šå®Ÿè¡Œ

å†…éƒ¨å‡¦ç†:
1. ã‚¿ã‚¹ã‚¯æƒ…å ±å–å¾—ï¼ˆDB + TASKãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
2. Workerå‰²å½“ãƒ»ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ï¼ˆIN_PROGRESSï¼‰
3. claude -p ã§ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ
4. REPORTä½œæˆ
5. ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚­ãƒ¥ãƒ¼è¿½åŠ 
6. ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ï¼ˆDONEï¼‰
7. ãƒ¬ãƒ“ãƒ¥ãƒ¼è‡ªå‹•å®Ÿè¡Œ â†’ ã‚¿ã‚¹ã‚¯COMPLETEDï¼ˆ--no-reviewã§ç„¡åŠ¹åŒ–å¯ï¼‰
"""

import argparse
import json
import logging
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict, Any

# ãƒ‘ã‚¹è¨­å®š
_current_dir = Path(__file__).resolve().parent
_package_root = _current_dir.parent
_project_root = _package_root.parent

if str(_package_root) not in sys.path:
    sys.path.insert(0, str(_package_root))

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

# å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    from utils.db import (
        get_connection, transaction, execute_query, fetch_one, fetch_all,
        row_to_dict, rows_to_dicts, DatabaseError
    )
    from utils.validation import (
        validate_project_name, validate_task_id,
        project_exists, task_exists, ValidationError
    )
    from utils.transition import (
        validate_transition, record_transition, TransitionError
    )
    from utils.path_validation import (
        safe_path_join, validate_path_components, PathValidationError
    )
    from config.db_config import get_project_paths
except ImportError as e:
    logger.error(f"å†…éƒ¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
    sys.exit(1)

# claude_cli ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆORDER_168: claude_runner â†’ claude_cliç§»è¡Œï¼‰
try:
    from utils.claude_cli import create_runner, ClaudeRunner, ClaudeResult
    CLAUDE_RUNNER_AVAILABLE = True
except ImportError:
    CLAUDE_RUNNER_AVAILABLE = False
    logger.warning("claude_cli ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚--skip-ai ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™ã€‚")

# Auto Recovery Engine (ORDER_109)
try:
    from worker.auto_recovery import AutoRecoveryEngine
    HAS_AUTO_RECOVERY = True
except ImportError:
    HAS_AUTO_RECOVERY = False

# æ¨©é™ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•åˆ¤å®šï¼ˆORDER_121ï¼‰
try:
    from worker.permission_resolver import PermissionResolver
    PERMISSION_RESOLVER_AVAILABLE = True
except ImportError:
    PERMISSION_RESOLVER_AVAILABLE = False


class WorkerExecutionError(Exception):
    """Workerå®Ÿè¡Œã‚¨ãƒ©ãƒ¼"""
    pass


# Workerå®Ÿè¡Œæ™‚ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨±å¯ãƒ„ãƒ¼ãƒ«
# claude -p ã® --allowedTools ã«æ¸¡ã•ã‚Œã‚‹
DEFAULT_WORKER_ALLOWED_TOOLS = [
    "Read", "Write", "Edit", "Glob", "Grep",
    "Bash", "WebSearch", "WebFetch",
    "TodoWrite", "Task", "NotebookEdit",
]


class WorkerExecutor:
    """Workerå‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹"""

    def __init__(
        self,
        project_id: str,
        task_id: str,
        *,
        dry_run: bool = False,
        skip_ai: bool = False,
        verbose: bool = False,
        timeout: int = 1800,
        model: Optional[str] = None,
        auto_review: bool = True,
        review_model: str = "sonnet",
        loop: bool = False,
        max_tasks: int = 100,
        is_rework: bool = False,
        rework_comment: Optional[str] = None,
        allowed_tools: Optional[list] = None,
    ):
        self.project_id = project_id
        # TASK_XXX å½¢å¼ã«æ­£è¦åŒ–
        self.task_id = f"TASK_{task_id}" if not task_id.startswith("TASK_") else task_id
        self.dry_run = dry_run
        self.skip_ai = skip_ai
        self.verbose = verbose
        self.timeout = timeout
        self.model = model
        self.auto_review = auto_review
        self.review_model = review_model
        self.loop = loop
        self.max_tasks = max_tasks
        self.is_rework = is_rework
        self.rework_comment = rework_comment
        # æ¨©é™ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•åˆ¤å®šãƒ•ãƒ©ã‚°: allowed_toolsãŒæœªæŒ‡å®šã®å ´åˆã€
        # _step_get_task_info()å®Œäº†å¾Œã«ã‚¿ã‚¹ã‚¯æƒ…å ±ã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•åˆ¤å®šã™ã‚‹
        self._needs_profile_resolution = (allowed_tools is None) and PERMISSION_RESOLVER_AVAILABLE
        self.allowed_tools = allowed_tools if allowed_tools is not None else DEFAULT_WORKER_ALLOWED_TOOLS.copy()
        self._resolved_profile: Optional[str] = None

        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ï¼ˆUSER_DATA_PATHçµŒç”±ï¼‰
        self.project_dir = get_project_paths(project_id)["base"]

        # å‡¦ç†çµæžœ
        self.results: Dict[str, Any] = {
            "task_id": self.task_id,
            "project_id": project_id,
            "steps": [],
            "success": False,
            "error": None,
            "is_rework": is_rework,
        }

        # ã‚¿ã‚¹ã‚¯æƒ…å ±ï¼ˆå¾Œã§è¨­å®šï¼‰
        self.task_info: Optional[Dict] = None
        self.order_id: Optional[str] = None
        self.worker_id: Optional[str] = None

        # claude_runner ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆå¾Œã§è¨­å®šï¼‰
        self.runner: Optional[ClaudeRunner] = None

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆIDï¼ˆå¾Œã§è¨­å®šï¼‰
        self.checkpoint_id: Optional[str] = None

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆID (ORDER_109)
        self.snapshot_id: Optional[str] = None

    def _log_step(self, step: str, status: str, detail: str = "") -> None:
        """ã‚¹ãƒ†ãƒƒãƒ—ãƒ­ã‚°ã‚’è¨˜éŒ²"""
        entry = {
            "step": step,
            "status": status,
            "detail": detail,
            "timestamp": datetime.now().isoformat(),
        }
        self.results["steps"].append(entry)
        if self.verbose:
            logger.info(f"[{step}] {status}: {detail}")

    def execute(self) -> Dict[str, Any]:
        """
        Workerå‡¦ç†ã‚’å®Ÿè¡Œ

        Returns:
            å‡¦ç†çµæžœã®è¾žæ›¸
        """
        try:
            # Step 1: ã‚¿ã‚¹ã‚¯æƒ…å ±å–å¾—
            self._step_get_task_info()

            # Step 2: Workerå‰²å½“ãƒ»ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°
            self._step_assign_worker()

            if self.dry_run:
                self._log_step("dry_run", "info", "ãƒ‰ãƒ©ã‚¤ãƒ©ãƒ³ãƒ¢ãƒ¼ãƒ‰ - ä»¥é™ã®å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—")
                self.results["success"] = True
                return self.results

            # Step 3: ã‚¿ã‚¹ã‚¯å®Ÿè¡Œï¼ˆAIï¼‰
            if not self.skip_ai:
                self._step_execute_task()

            # Step 3.5: è‡ªå·±æ¤œè¨¼ï¼‹è‡ªå·±ä¿®æ­£ãƒ«ãƒ¼ãƒ—
            if not self.skip_ai:
                self._step_self_verification()

            # Step 4: REPORTä½œæˆ
            self._step_create_report()

            # Step 4.5: é™çš„è§£æžï¼ˆæˆæžœç‰©ã«å¯¾ã—ã¦è‡ªå‹•å®Ÿè¡Œï¼‰
            if not self.skip_ai:
                self._step_static_analysis()

            # Step 4.6: ç ´å£Šçš„SQLæ¤œå‡ºï¼ˆæˆæžœç‰©ã«å¯¾ã—ã¦è‡ªå‹•å®Ÿè¡Œï¼‰
            if not self.skip_ai:
                self._step_destructive_sql_check()

            # Step 5: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ï¼ˆDONEï¼‰
            self._step_update_status_done()

            # Step 6.5: ãƒã‚°ä¿®æ­£ã‚¿ã‚¹ã‚¯ã®è‡ªå‹•è¨˜éŒ²ï¼ˆORDER_007ï¼‰
            if not self.skip_ai:
                self._step_record_bug_fix()

            # Step 7: è‡ªå‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼ˆ--auto-review æŒ‡å®šæ™‚ï¼‰
            # NOTE: ORDER_132ã§ç„¡åŠ¹åŒ– - ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯review_workerã§åˆ¥ãƒ—ãƒ­ã‚»ã‚¹å®Ÿè¡Œ
            # if self.auto_review:
            #     review_result = self._step_auto_review()
            #     self.results["review_result"] = review_result
            #     if review_result.get("success"):
            #         self._log_step("auto_review", "success", f"verdict={review_result.get('verdict')}")
            #
            #         # ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒæ‰¿èªã•ã‚Œã¦ã‚¿ã‚¹ã‚¯ãŒCOMPLETEDã«ãªã£ãŸå ´åˆã€å¾Œç¶šã‚¿ã‚¹ã‚¯ã‚’ãƒã‚§ãƒƒã‚¯
            #         verdict = review_result.get("verdict", "")
            #         if verdict == "APPROVE":
            #             self._step_check_successor_tasks()
            #
            #         # Step 7.5: ãƒã‚°å­¦ç¿’ãƒ•ãƒƒã‚¯
            #         self._step_bug_learning(review_result)
            #     else:
            #         self._log_step("auto_review", "warning", review_result.get("error", "ãƒ¬ãƒ“ãƒ¥ãƒ¼å¤±æ•—"))
            if self.auto_review:
                self._log_step("auto_review", "skipped", "ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯review_workerã§å®Ÿè¡Œã•ã‚Œã¾ã™")

            # Step 8: æ¬¡ã‚¿ã‚¹ã‚¯æ¤œå‡ºï¼ˆ--loop æŒ‡å®šæ™‚ï¼‰
            if self.loop:
                next_task = self._get_next_queued_task()
                if next_task:
                    self.results["next_task"] = next_task
                    self._log_step("next_task", "found", f"next={next_task}")
                else:
                    self.results["next_task"] = None
                    self._log_step("next_task", "none", "QUEUEDã‚¿ã‚¹ã‚¯ãªã—")

            self.results["success"] = True
            self._log_step("complete", "success", "Workerå‡¦ç†å®Œäº†")

        except WorkerExecutionError as e:
            self.results["error"] = str(e)
            self._log_step("error", "failed", str(e))
            # è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‹ãƒªãƒˆãƒ©ã‚¤åˆ¤å®š
            self._handle_execution_failure(e)
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾
            self._release_locks_on_error()
        except Exception as e:
            self.results["error"] = f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}"
            self._log_step("error", "failed", str(e))
            if self.verbose:
                logger.exception("è©³ç´°ã‚¨ãƒ©ãƒ¼")
            # è‡ªå‹•ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‹ãƒªãƒˆãƒ©ã‚¤åˆ¤å®š
            self._handle_execution_failure(e)
            # ã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾
            self._release_locks_on_error()

        return self.results

    def _release_locks_on_error(self) -> None:
        """ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚ã«ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾"""
        try:
            from utils.file_lock import FileLockManager
            FileLockManager.release_locks(self.project_id, self.task_id)
            self._log_step("file_lock_release", "success", "ã‚¨ãƒ©ãƒ¼æ™‚ãƒ­ãƒƒã‚¯è§£æ”¾")

            # NOTE: ã‚¨ãƒ©ãƒ¼æ™‚ã® auto_kick ã¯å‰Šé™¤ï¼ˆBACKLOG_167ä¿®æ­£ï¼‰
            # ã‚¨ãƒ©ãƒ¼çŠ¶æ…‹ã®ã‚¿ã‚¹ã‚¯ã§å¾Œç¶šã‚’è§£é™¤ã™ã¹ãã§ã¯ãªã„ã€‚
            # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ‰¿èªå¾Œã«æ­£è¦ãƒ•ãƒ­ãƒ¼ã§è§£é™¤ã™ã‚‹ã€‚

        except ImportError:
            pass
        except Exception as e:
            self._log_step("file_lock_release", "warning", f"ã‚¨ãƒ©ãƒ¼æ™‚ãƒ­ãƒƒã‚¯è§£æ”¾å¤±æ•—: {e}")

    def _handle_execution_failure(self, error: Exception) -> None:
        """
        Handle task execution failure with auto-rollback and retry.

        This method is called when _step_execute_task() raises an exception.
        It performs:
        1. Rollback to checkpoint if available
        2. Record the failure in INCIDENTS table
        3. Check retry eligibility
        4. Update task status (REWORK if retryable, REJECTED if limit exceeded)

        ORDER_109: AutoRecoveryEngineçµ±åˆ
        - HAS_AUTO_RECOVERY=True: AutoRecoveryEngineçµŒç”±ã§ã‚¨ãƒ©ãƒ¼åˆ†æžâ†’æˆ¦ç•¥æ±ºå®šâ†’ãƒªã‚«ãƒãƒª
        - HAS_AUTO_RECOVERY=False: å¾“æ¥ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰

        Args:
            error: The exception that caused the failure
        """
        self._log_step("self_healing", "start", f"Handling execution failure: {error}")

        # --- ORDER_109: AutoRecoveryEngineçµ±åˆ ---
        if HAS_AUTO_RECOVERY:
            try:
                # db_path: AutoRecoveryEngineå´ã§ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè§£æ±ºã™ã‚‹ãŸã‚Noneæ¸¡ã—
                recovery_engine = AutoRecoveryEngine(
                    db_path=None,
                    project_id=self.project_id,
                )

                # ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯å–å¾—
                import traceback
                tb_text = traceback.format_exc()
                error_msg = str(error)

                # ãƒ¯ãƒ³ã‚·ãƒ§ãƒƒãƒˆãƒªã‚«ãƒãƒªå®Ÿè¡Œ
                result = recovery_engine.recover(
                    task_id=self.task_id,
                    order_id=self.order_id,
                    error_message=error_msg,
                    traceback_text=tb_text,
                    snapshot_id=self.snapshot_id,
                    checkpoint_id=self.checkpoint_id,
                )

                self.results["auto_recovery_result"] = {
                    "success": result.success,
                    "action_taken": result.action_taken.value,
                    "message": result.message,
                    "next_status": result.next_status,
                    "retry_count": result.retry_count,
                }
                self.results["self_healing_action"] = result.next_status

                self._log_step(
                    "self_healing", "complete",
                    f"AutoRecovery: action={result.action_taken.value}, "
                    f"next_status={result.next_status}, "
                    f"success={result.success}"
                )
                return  # AutoRecoveryEngineå‡¦ç†å®Œäº†ã€æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—

            except Exception as ar_error:
                self._log_step(
                    "self_healing", "warning",
                    f"AutoRecoveryEngine failed, falling back to legacy: {ar_error}"
                )
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ä»¥ä¸‹ã®æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ã‚’å®Ÿè¡Œ

        # --- æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰ ---
        # Step 1: Attempt rollback if checkpoint exists
        rollback_success = False
        if self.checkpoint_id:
            try:
                from rollback.auto_rollback import rollback_to_checkpoint

                rollback_result = rollback_to_checkpoint(
                    project_id=self.project_id,
                    task_id=self.task_id,
                    checkpoint_id=self.checkpoint_id,
                    verbose=self.verbose,
                )

                if rollback_result.success:
                    rollback_success = True
                    self._log_step(
                        "self_healing_rollback", "success",
                        f"Rolled back to checkpoint={self.checkpoint_id}, "
                        f"db_restored={rollback_result.db_restored}"
                    )
                    self.results["rollback_result"] = rollback_result.to_dict()
                else:
                    self._log_step(
                        "self_healing_rollback", "warning",
                        f"Rollback returned failure: {rollback_result.error_message}"
                    )
            except ImportError:
                self._log_step(
                    "self_healing_rollback", "skip",
                    "rollback module not available"
                )
            except Exception as rb_error:
                self._log_step(
                    "self_healing_rollback", "warning",
                    f"Rollback error: {rb_error}"
                )
        else:
            self._log_step(
                "self_healing_rollback", "skip",
                "No checkpoint_id available for rollback"
            )

        # Step 2: Record the failure in INCIDENTS table
        incident_id = None
        try:
            from incidents.create import create_incident

            incident_id = create_incident(
                project_id=self.project_id,
                task_id=self.task_id,
                category="WORKER_FAILURE",
                description=f"Task execution failed: {error}",
                root_cause=str(type(error).__name__),
                severity="HIGH",
                order_id=self.order_id,
            )
            self._log_step(
                "self_healing_incident", "success",
                f"Incident recorded: {incident_id}"
            )
            self.results["failure_incident_id"] = incident_id
        except ImportError:
            self._log_step(
                "self_healing_incident", "skip",
                "incidents module not available"
            )
        except Exception as inc_error:
            self._log_step(
                "self_healing_incident", "warning",
                f"Failed to record incident: {inc_error}"
            )

        # Step 3: Check retry eligibility
        try:
            from retry.retry_handler import RetryHandler

            handler = RetryHandler(
                self.project_id,
                self.task_id,
                max_retries=2,
                verbose=self.verbose,
            )
            retry_result = handler.prepare_retry()
            self.results["retry_result"] = retry_result.to_dict()

            # Step 4: Update task status
            try:
                from task.update import update_task

                if retry_result.should_retry:
                    # Set to REWORK for retry
                    update_task(
                        self.project_id,
                        self.task_id,
                        status="REWORK",
                        role="PM",
                        reason=(
                            f"Auto-recovery: execution failed ({error}), "
                            f"retry {retry_result.retry_count + 1}/"
                            f"{retry_result.max_retries}"
                        ),
                    )
                    self._log_step(
                        "self_healing_status", "success",
                        f"Task set to REWORK for retry "
                        f"(attempt {retry_result.retry_count + 1}/"
                        f"{retry_result.max_retries})"
                    )
                    self.results["self_healing_action"] = "REWORK"
                else:
                    # Retry limit exceeded - mark as REJECTED
                    update_task(
                        self.project_id,
                        self.task_id,
                        status="REJECTED",
                        role="PM",
                        reason=(
                            f"Auto-recovery: retry limit exceeded "
                            f"({retry_result.retry_count}/"
                            f"{retry_result.max_retries}), "
                            f"error: {error}"
                        ),
                    )
                    self._log_step(
                        "self_healing_status", "warning",
                        f"Task set to REJECTED (retry limit exceeded: "
                        f"{retry_result.retry_count}/{retry_result.max_retries})"
                    )
                    self.results["self_healing_action"] = "REJECTED"

            except Exception as status_error:
                self._log_step(
                    "self_healing_status", "warning",
                    f"Failed to update task status: {status_error}"
                )

        except ImportError:
            self._log_step(
                "self_healing_retry", "skip",
                "retry module not available"
            )
        except Exception as retry_error:
            self._log_step(
                "self_healing_retry", "warning",
                f"Retry evaluation error: {retry_error}"
            )

        self._log_step(
            "self_healing", "complete",
            f"rollback={'OK' if rollback_success else 'SKIP'}, "
            f"action={self.results.get('self_healing_action', 'UNKNOWN')}"
        )

    def _step_get_task_info(self) -> None:
        """Step 1: ã‚¿ã‚¹ã‚¯æƒ…å ±ã‚’å–å¾—"""
        self._log_step("get_task_info", "start", self.task_id)

        conn = get_connection()
        try:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå­˜åœ¨ç¢ºèª
            if not project_exists(conn, self.project_id):
                raise WorkerExecutionError(f"ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.project_id}")

            # ã‚¿ã‚¹ã‚¯å­˜åœ¨ç¢ºèª
            if not task_exists(conn, self.task_id, self.project_id):
                raise WorkerExecutionError(f"ã‚¿ã‚¹ã‚¯ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.task_id}")

            # ã‚¿ã‚¹ã‚¯æƒ…å ±å–å¾—
            task = fetch_one(
                conn,
                """
                SELECT t.*, o.title as order_title
                FROM tasks t
                LEFT JOIN orders o ON t.order_id = o.id AND t.project_id = o.project_id
                WHERE t.id = ? AND t.project_id = ?
                """,
                (self.task_id, self.project_id)
            )

            self.task_info = row_to_dict(task)
            self.order_id = self.task_info.get("order_id")
            self.results["task_info"] = self.task_info

            # æŽ¨å¥¨ãƒ¢ãƒ‡ãƒ«è¨­å®šï¼ˆREWORKå›žæ•°ã«å¿œã˜ãŸè‡ªå‹•æ˜‡æ ¼ã‚’å«ã‚€ï¼‰
            recommended = self.task_info.get("recommended_model", "").lower()
            reject_count = self.task_info.get("reject_count", 0)

            # REWORK 2å›žç›®ä»¥é™ã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•æ˜‡æ ¼
            if reject_count >= 2:
                # Sonnet â†’ Opus ã«è‡ªå‹•æ˜‡æ ¼
                if recommended in ("haiku", "sonnet"):
                    original_model = recommended
                    recommended = "opus"
                    self._log_step(
                        "model_upgrade",
                        "info",
                        f"REWORK {reject_count}å›žç›®: ãƒ¢ãƒ‡ãƒ«è‡ªå‹•æ˜‡æ ¼ {original_model} â†’ opus"
                    )

                    # ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°è¨˜éŒ²
                    try:
                        from escalation.log_escalation import log_escalation, EscalationType
                        log_escalation(
                            project_id=self.project_id,
                            task_id=self.task_id,
                            escalation_type=EscalationType.MODEL_UPGRADE,
                            description=f"REWORK {reject_count}å›žç›®: ãƒ¢ãƒ‡ãƒ«è‡ªå‹•æ˜‡æ ¼ã‚’å®Ÿæ–½",
                            order_id=self.order_id,
                            metadata={
                                "from_model": original_model,
                                "to_model": "opus",
                                "rework_count": reject_count,
                            }
                        )
                    except Exception as e:
                        logger.warning(f"ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°è¨˜éŒ²å¤±æ•—: {e}")

            if not self.model and recommended in ("haiku", "sonnet", "opus"):
                self.model = recommended
            elif not self.model:
                self.model = "sonnet"

            self._log_step("get_task_info", "success", f"order={self.order_id}, model={self.model}, reject_count={reject_count}")

        finally:
            conn.close()

        # æ¨©é™ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•åˆ¤å®šï¼ˆallowed_toolsãŒæœªæŒ‡å®šã®å ´åˆã®ã¿ï¼‰
        if self._needs_profile_resolution and self.task_info:
            try:
                resolver = PermissionResolver()
                self._resolved_profile = resolver.resolve(self.task_info)
                resolved_tools = resolver.resolve_tools(self.task_info)
                self.allowed_tools = resolved_tools
                self._log_step(
                    "permission_profile", "success",
                    f"profile={self._resolved_profile}, tools={len(resolved_tools)}å€‹: {resolved_tools}"
                )
            except Exception as e:
                # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«è§£æ±ºå¤±æ•—æ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®allowed_toolsã‚’ç¶­æŒ
                self._log_step(
                    "permission_profile", "warning",
                    f"ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆ¤å®šå¤±æ•—ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨©é™ã‚’ä½¿ç”¨: {e}"
                )

    def _step_assign_worker(self) -> None:
        """Step 2: Workerå‰²å½“ãƒ»ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°"""
        mode_label = "ãƒªãƒ¯ãƒ¼ã‚¯" if self.is_rework else "é€šå¸¸"
        self._log_step("assign_worker", "start", f"mode={mode_label}")

        # Workerè­˜åˆ¥å­ã‚’å–å¾—
        try:
            from worker.assign import get_next_worker
            self.worker_id = get_next_worker(self.project_id)
        except Exception as e:
            # Workerå–å¾—å¤±æ•—æ™‚ã¯ "Auto" ã‚’ä½¿ç”¨
            self.worker_id = "Auto"
            logger.warning(f"Workerè­˜åˆ¥å­å–å¾—å¤±æ•—ã€'Auto' ã‚’ä½¿ç”¨: {e}")

        self.results["worker_id"] = self.worker_id

        if self.dry_run:
            self._log_step("assign_worker", "dry_run", f"worker={self.worker_id}")
            return

        # REWORKå†å®Ÿè¡Œæ™‚ã®ãƒ­ãƒƒã‚¯æ•´åˆå‡¦ç†
        # IN_PROGRESSé·ç§»å‰ã«æ—¢å­˜ãƒ­ãƒƒã‚¯ã‚’ä¸€æ—¦è§£æ”¾ã—ã¦ã‹ã‚‰å†å–å¾—
        # ã“ã‚Œã«ã‚ˆã‚ŠREWORKãƒ«ãƒ¼ãƒ—ã§ã®ãƒ­ãƒƒã‚¯è“„ç©ã‚’é˜²æ­¢ï¼ˆBUG_008å¯¾ç­–ï¼‰
        current_status = self.task_info.get("status", "") if self.task_info else ""
        if current_status in ("REWORK", "IN_PROGRESS"):
            try:
                from utils.file_lock import FileLockManager

                # æ—¢å­˜ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾
                FileLockManager.release_locks(self.project_id, self.task_id)
                self._log_step(
                    "file_lock_cleanup",
                    "success",
                    f"REWORKå†å®Ÿè¡Œå‰ã«ãƒ­ãƒƒã‚¯è§£æ”¾ (status={current_status})"
                )
            except ImportError:
                self._log_step("file_lock_cleanup", "skip", "FileLockManageråˆ©ç”¨ä¸å¯")
            except Exception as e:
                # ãƒ­ãƒƒã‚¯è§£æ”¾å¤±æ•—ã¯è­¦å‘Šã®ã¿ï¼ˆãƒ­ãƒƒã‚¯ãŒå­˜åœ¨ã—ãªã„å ´åˆã‚‚å«ã‚€ï¼‰
                self._log_step("file_lock_cleanup", "warning", f"ãƒ­ãƒƒã‚¯è§£æ”¾ã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã®å–å¾—ã‚’è©¦ã¿ã‚‹
        try:
            from utils.file_lock import FileLockManager

            # ã‚¿ã‚¹ã‚¯ã®å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—
            target_files_json = self.task_info.get("target_files")
            target_files = FileLockManager.parse_target_files(target_files_json)

            if target_files:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã‚’å–å¾—
                lock_acquired = FileLockManager.acquire_locks(
                    self.project_id,
                    self.task_id,
                    target_files
                )

                if not lock_acquired:
                    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ç«¶åˆ
                    conflicts = FileLockManager.check_conflicts(self.project_id, target_files)
                    blocking_tasks = list(set(c["task_id"] for c in conflicts))
                    raise WorkerExecutionError(
                        f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ç«¶åˆ: ã‚¿ã‚¹ã‚¯ {', '.join(blocking_tasks)} ãŒå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒƒã‚¯ä¸­ã§ã™"
                    )

                self._log_step("file_lock", "success", f"ãƒ­ãƒƒã‚¯å–å¾—: {len(target_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
                self.results["locked_files"] = target_files
            else:
                self._log_step("file_lock", "skip", "å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«æœªæŒ‡å®š")

        except ImportError:
            self._log_step("file_lock", "skip", "FileLockManageråˆ©ç”¨ä¸å¯")
        except Exception as e:
            self._log_step("file_lock", "warning", f"ãƒ­ãƒƒã‚¯å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ­ãƒƒã‚¯å–å¾—å¤±æ•—ã¯è‡´å‘½çš„ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦æ‰±ã†
            raise WorkerExecutionError(f"ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯å–å¾—å¤±æ•—: {e}")

        # ç¾åœ¨ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’å–å¾—
        current_status = self.task_info.get("status", "") if self.task_info else ""

        # IN_PROGRESS/REWORKå†å®Ÿè¡Œå¯¾å¿œ: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¶­æŒã—ã¦assigneeã®ã¿æ›´æ–°
        # REWORKæ™‚ã¯REWORKã®ã¾ã¾ä½œæ¥­ã—ã€å®Œäº†æ™‚ã«DONEã¸é·ç§»ã™ã‚‹
        if current_status in ("IN_PROGRESS", "REWORK"):
            mode_str = "REWORKï¼ˆãƒªãƒ¯ãƒ¼ã‚¯ï¼‰" if current_status == "REWORK" else "IN_PROGRESSï¼ˆå†å®Ÿè¡Œï¼‰"
            self._log_step(
                "assign_worker",
                "info",
                f"ã‚¿ã‚¹ã‚¯ã¯{mode_str}çŠ¶æ…‹ - ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¶­æŒãƒ¢ãƒ¼ãƒ‰ (assignee={self.task_info.get('assignee')})"
            )

            # WorkerãŒç•°ãªã‚‹å ´åˆã¯è­¦å‘Šã‚’å‡ºã™ï¼ˆåˆ¥WorkerãŒå®Ÿè¡Œä¸­ã®å¯èƒ½æ€§ï¼‰
            current_assignee = self.task_info.get("assignee")
            if current_assignee and current_assignee != self.worker_id:
                self._log_step(
                    "assign_worker",
                    "warning",
                    f"WorkerãŒå¤‰æ›´ã•ã‚Œã¾ã™: {current_assignee} â†’ {self.worker_id}"
                )

            # assigneeã®ã¿ã‚’æ›´æ–°ï¼ˆstatusé·ç§»ãªã—ï¼‰
            from task.update import update_task
            try:
                update_task(
                    self.project_id,
                    self.task_id,
                    assignee=self.worker_id,
                    role="Worker",
                    reason=f"{current_status}ç¶­æŒ - Workerå‰²å½“æ›´æ–°",
                )
                self._log_step(
                    "assign_worker",
                    "success",
                    f"worker={self.worker_id} ({mode_str}ãƒ¢ãƒ¼ãƒ‰)"
                )
            except Exception as e:
                # assigneeæ›´æ–°å¤±æ•—æ™‚ã‚‚ç¶šè¡Œå¯èƒ½ï¼ˆæ—¢å­˜ã®assigneeã§å®Ÿè¡Œï¼‰
                self._log_step(
                    "assign_worker",
                    "warning",
                    f"Workerå‰²å½“æ›´æ–°å¤±æ•—: {e} - æ—¢å­˜assigneeã§ç¶šè¡Œ"
                )

            # å†å®Ÿè¡Œãƒ•ãƒ©ã‚°ã‚’ã‚»ãƒƒãƒˆ
            self.results["is_reexecution"] = True
            return

        # é€šå¸¸ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°ï¼ˆQUEUED â†’ IN_PROGRESSï¼‰
        from task.update import update_task

        try:
            update_task(
                self.project_id,
                self.task_id,
                status="IN_PROGRESS",
                assignee=self.worker_id,
                role="Worker",
            )
            self._log_step("assign_worker", "success", f"worker={self.worker_id}")
        except TransitionError as e:
            # äºˆæœŸã—ãªã„é·ç§»ã‚¨ãƒ©ãƒ¼
            raise WorkerExecutionError(f"ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°å¤±æ•—: {e}")

    def _create_checkpoint(self) -> None:
        """
        ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆï¼ˆã‚¿ã‚¹ã‚¯å®Ÿè¡Œå‰ï¼‰

        DBã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã¨ãƒ•ã‚¡ã‚¤ãƒ«çŠ¶æ…‹ã‚’ä¿å­˜ã—ã¾ã™ã€‚
        å¤±æ•—æ™‚ã¯è­¦å‘Šãƒ­ã‚°ã‚’å‡ºåŠ›ã—ã¾ã™ãŒã€ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã¯ç¶šè¡Œã—ã¾ã™ã€‚
        """
        self._log_step("create_checkpoint", "start", f"task={self.task_id}")

        try:
            from checkpoint.create import create_checkpoint, CheckpointError

            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆ
            checkpoint_id = create_checkpoint(
                project_id=self.project_id,
                task_id=self.task_id,
                order_id=self.order_id,
                verbose=self.verbose
            )

            self.checkpoint_id = checkpoint_id
            self.results["checkpoint_id"] = checkpoint_id
            self._log_step("create_checkpoint", "success", f"checkpoint={checkpoint_id}")

        except ImportError:
            self._log_step("create_checkpoint", "skip", "checkpoint module not available")
        except Exception as e:
            # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆå¤±æ•—ã¯è­¦å‘Šã®ã¿ï¼ˆã‚¿ã‚¹ã‚¯å®Ÿè¡Œã¯ç¶šè¡Œï¼‰
            self._log_step("create_checkpoint", "warning", f"ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆå¤±æ•—: {e}")
            if self.verbose:
                logger.exception("create_checkpointè©³ç´°ã‚¨ãƒ©ãƒ¼")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆ (ORDER_109)
        try:
            from worker.snapshot_manager import SnapshotManager
            sm = SnapshotManager(self.project_id)
            # target_filesã®å–å¾—: ã‚¿ã‚¹ã‚¯æƒ…å ±ã®target_filesã‹ã€ORDERæˆæžœç‰©ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            target_files = None
            if self.task_info and self.task_info.get("target_files"):
                tf = self.task_info["target_files"]
                target_files = tf.split(",") if isinstance(tf, str) else tf
            self.snapshot_id = sm.create_snapshot(self.task_id, self.order_id, target_files)
            self.results["snapshot_id"] = self.snapshot_id
            self._log_step("create_snapshot", "success", f"snapshot={self.snapshot_id}")
        except ImportError:
            self._log_step("create_snapshot", "skip", "snapshot_manager not available")
        except Exception as e:
            self._log_step("create_snapshot", "warning", f"ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆä½œæˆå¤±æ•—: {e}")

    def _check_migration_safety(self) -> None:
        """
        ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œå‰ã®å®‰å…¨ãƒã‚§ãƒƒã‚¯

        Workerå®Ÿè¡Œä¸­ã«ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å‘¼ã³å‡ºã™éš›ã®å®‰å…¨ã‚¬ãƒ¼ãƒ‰ã€‚
        ä»–ã®WorkerãŒå®Ÿè¡Œä¸­ã®å ´åˆã€ã‚¹ã‚­ãƒ¼ãƒžå¤‰æ›´ã¯å±é™ºãªãŸã‚è­¦å‘Šã¾ãŸã¯ãƒ–ãƒ­ãƒƒã‚¯ã™ã‚‹ã€‚

        Raises:
            WorkerExecutionError: ä»–ã®WorkerãŒå®Ÿè¡Œä¸­ã§å®‰å…¨ã§ãªã„å ´åˆ
        """
        # ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‹ã©ã†ã‹ã‚’åˆ¤å®š
        task_title = self.task_info.get("title", "").lower()
        task_desc = self.task_info.get("description", "").lower()

        migration_keywords = [
            "migration", "ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", "schema", "ã‚¹ã‚­ãƒ¼ãƒž",
            "alter table", "drop table", "create table",
            "pragma", "foreign_keys"
        ]

        is_migration_task = any(
            keyword in task_title or keyword in task_desc
            for keyword in migration_keywords
        )

        if not is_migration_task:
            # ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã§ãªã„å ´åˆã¯ãƒã‚§ãƒƒã‚¯ä¸è¦
            return

        self._log_step("migration_safety_check", "start", "ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯æ¤œå‡º")

        # ä»–ã®Workerå®Ÿè¡Œä¸­ã‚¿ã‚¹ã‚¯ã‚’æ¤œå‡º
        conn = get_connection()
        try:
            running_tasks = fetch_all(
                conn,
                """
                SELECT t.id, t.project_id, t.title, t.assignee, t.updated_at
                FROM tasks t
                WHERE t.status = 'IN_PROGRESS'
                  AND t.id != ?
                ORDER BY t.updated_at DESC
                """,
                (self.task_id,)
            )

            if running_tasks:
                # å®Ÿè¡Œä¸­ã‚¿ã‚¹ã‚¯ãŒã‚ã‚‹å ´åˆã¯è­¦å‘Š
                self._log_step(
                    "migration_safety_check",
                    "warning",
                    f"ä»–ã®WorkerãŒå®Ÿè¡Œä¸­: {len(running_tasks)}ä»¶"
                )

                for task in running_tasks:
                    logger.warning(
                        f"  - {task['id']} ({task['project_id']}): {task['title']} "
                        f"[assignee={task['assignee']}, updated={task['updated_at']}]"
                    )

                # è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                logger.warning(
                    "âš ï¸  ä»–ã®WorkerãŒå®Ÿè¡Œä¸­ã§ã™ã€‚ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œã¯ã‚¹ã‚­ãƒ¼ãƒžå¤‰æ›´ã«ã‚ˆã‚Š"
                    "ä»–ã®ã‚¿ã‚¹ã‚¯ãŒå¤±æ•—ã™ã‚‹åŽŸå› ã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
                )
                logger.warning(
                    "ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã§MigrationRunnerã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€"
                    "MigrationRunnerãŒè‡ªå‹•çš„ã«å®‰å…¨ãƒã‚§ãƒƒã‚¯ã‚’è¡Œã„ã¾ã™ã€‚"
                )

                # ã‚¨ãƒ©ãƒ¼ã¨ã—ã¦æ‰±ã‚ãšè­¦å‘Šã®ã¿ï¼ˆMigrationRunnerãŒæœ€çµ‚åˆ¤æ–­ã‚’è¡Œã†ï¼‰
                self._log_step(
                    "migration_safety_check",
                    "warning",
                    "ä»–Workerå®Ÿè¡Œä¸­ - MigrationRunnerã«ã‚ˆã‚‹æœ€çµ‚åˆ¤æ–­å¾…ã¡"
                )
            else:
                self._log_step(
                    "migration_safety_check",
                    "success",
                    "ä»–ã®Workerå®Ÿè¡Œãªã— - å®‰å…¨"
                )

        finally:
            conn.close()

    def _step_execute_task(self) -> None:
        """Step 3: claude -p ã§ã‚¿ã‚¹ã‚¯å®Ÿè¡Œ"""
        if not CLAUDE_RUNNER_AVAILABLE:
            self._log_step("execute_task", "skip", "claude_runner åˆ©ç”¨ä¸å¯")
            return

        profile_label = f", profile={self._resolved_profile}" if self._resolved_profile else ""
        self._log_step("execute_task", "start", f"model={self.model}{profile_label}, allowed_tools={len(self.allowed_tools)}å€‹")

        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä½œæˆï¼ˆã‚¿ã‚¹ã‚¯å®Ÿè¡Œå‰ï¼‰
        self._create_checkpoint()

        # ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®‰å…¨ãƒã‚§ãƒƒã‚¯
        self._check_migration_safety()

        # claude_runner åˆæœŸåŒ–
        self.runner = create_runner(
            model=self.model,
            max_turns=50,
            timeout_seconds=self.timeout,
        )

        # TASKãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’å–å¾—
        task_content = self._read_task_file()

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰
        prompt = self._build_execution_prompt(task_content)

        # claude -p å®Ÿè¡Œ
        result = self.runner.run(prompt)

        if not result.success:
            raise WorkerExecutionError(f"ã‚¿ã‚¹ã‚¯å®Ÿè¡Œã«å¤±æ•—: {result.error_message}")

        self.results["execution_result"] = result.result_text
        self.results["cost_usd"] = result.cost_usd

        self._log_step(
            "execute_task",
            "success",
            f"cost=${result.cost_usd:.4f}" if result.cost_usd else ""
        )

    def _step_self_verification(self) -> None:
        """Step 3.5: æˆæžœç‰©ã®è‡ªå·±æ¤œè¨¼ï¼‹è‡ªå·±ä¿®æ­£ãƒ«ãƒ¼ãƒ—

        Workerå®Ÿè¡Œå®Œäº†å¾Œã®æˆæžœç‰©ã«å¯¾ã—ã¦lint/test/åž‹ãƒã‚§ãƒƒã‚¯ã‚’è‡ªå‹•å®Ÿè¡Œã—ã€
        å¤±æ•—æ™‚ã¯æœ€å¤§3å›žã®è‡ªå·±ä¿®æ­£ãƒ«ãƒ¼ãƒ—ã‚’è¡Œã†ã€‚
        æ¤œè¨¼å¯¾è±¡ãŒãªã„å ´åˆã‚„æ¤œè¨¼ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ããªã„å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã€‚
        """
        self._log_step("self_verification", "start", "")

        try:
            from worker.self_verification import SelfVerificationRunner
        except ImportError:
            self._log_step("self_verification", "skip", "self_verification ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åˆ©ç”¨ä¸å¯")
            return

        # å®Ÿè¡Œçµæžœã‹ã‚‰æˆæžœç‰©ãƒ‘ã‚¹ã‚’å–å¾—
        exec_result = self.results.get("execution_result", "")
        try:
            result_data = json.loads(exec_result)
            artifacts = result_data.get("artifacts", [])
        except (json.JSONDecodeError, AttributeError, TypeError):
            self._log_step("self_verification", "skip", "æˆæžœç‰©ãƒ‘ã‚¹å–å¾—ä¸å¯")
            return

        if not artifacts:
            self._log_step("self_verification", "skip", "æˆæžœç‰©ãªã—")
            return

        # æ¤œè¨¼ãƒ©ãƒ³ãƒŠãƒ¼åˆæœŸåŒ–
        runner = SelfVerificationRunner(
            project_dir=self.project_dir,
            artifacts=artifacts,
            timeout=120,
        )

        # ãƒ„ãƒ¼ãƒ«æ¤œå‡º
        tools = runner.detect_tools()
        if not tools.lint and not tools.test and not tools.typecheck:
            self._log_step("self_verification", "skip", "æ¤œè¨¼ãƒ„ãƒ¼ãƒ«æœªæ¤œå‡º")
            return

        # è‡ªå·±ä¿®æ­£ãƒ«ãƒ¼ãƒ—ï¼ˆæœ€å¤§3å›žï¼‰
        MAX_FIX_ITERATIONS = 3
        verification_history = []
        last_result = None

        for iteration in range(MAX_FIX_ITERATIONS + 1):
            # æ¤œè¨¼å®Ÿè¡Œ
            vresult = runner.run_verification()
            last_result = vresult
            verification_history.append({
                "iteration": iteration,
                "success": vresult.success,
                "checks": [c.to_dict() for c in vresult.checks],
                "skipped": vresult.skipped_checks,
                "duration": round(vresult.duration_seconds, 2),
            })

            if vresult.success:
                self._log_step(
                    "self_verification", "success",
                    f"å…¨æ¤œè¨¼ãƒ‘ã‚¹ï¼ˆè©¦è¡Œ{iteration + 1}å›žç›®ï¼‰"
                )
                break

            if iteration >= MAX_FIX_ITERATIONS:
                self._log_step(
                    "self_verification", "warning",
                    f"è‡ªå·±ä¿®æ­£ä¸Šé™åˆ°é”ï¼ˆ{MAX_FIX_ITERATIONS}å›žï¼‰- PMãƒ¬ãƒ“ãƒ¥ãƒ¼ã¸ã‚¨ã‚¹ã‚«ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³"
                )
                break

            # è‡ªå·±ä¿®æ­£ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆï¼‹å®Ÿè¡Œ
            if not CLAUDE_RUNNER_AVAILABLE or self.runner is None:
                self._log_step("self_verification", "warning", "claude_runneråˆ©ç”¨ä¸å¯ - è‡ªå·±ä¿®æ­£ã‚¹ã‚­ãƒƒãƒ—")
                break

            task_content = self._read_task_file()
            fix_prompt = runner.build_fix_prompt(vresult, task_content)

            self._log_step(
                "self_verification", "info",
                f"è‡ªå·±ä¿®æ­£å®Ÿè¡Œ (è©¦è¡Œ{iteration + 2}å›žç›®)"
            )

            try:
                fix_result = self.runner.run(fix_prompt)
                if not fix_result.success:
                    self._log_step("self_verification", "warning", "è‡ªå·±ä¿®æ­£å®Ÿè¡Œå¤±æ•—")
                    break
                # ä¿®æ­£çµæžœã§ execution_result ã‚’æ›´æ–°
                self.results["execution_result"] = fix_result.result_text
            except Exception as e:
                self._log_step("self_verification", "warning", f"è‡ªå·±ä¿®æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")
                break

        # æ¤œè¨¼çµæžœã‚’ä¿å­˜
        self.results["verification"] = {
            "final_success": last_result.success if last_result else False,
            "total_iterations": len(verification_history),
            "fix_attempts": max(0, len(verification_history) - 1),
            "history": verification_history,
        }

        self._log_step(
            "self_verification", "complete",
            f"æ¤œè¨¼å®Œäº†: success={last_result.success if last_result else False}, "
            f"iterations={len(verification_history)}"
        )

    def _read_task_file(self) -> str:
        """TASKãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€"""
        # ORDERé…ä¸‹ã®TASKãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        if self.order_id:
            # ãƒ‘ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¤œè¨¼ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹æ··å…¥é˜²æ­¢ï¼‰
            validate_path_components(self.order_id, self.task_id)

            task_file = safe_path_join(
                self.project_dir, "RESULT", self.order_id, "04_TASKS",
                f"{self.task_id}.md"
            )
            if task_file.exists():
                return task_file.read_text(encoding="utf-8")

        # STAFFINGã‹ã‚‰æ¤œç´¢
        if self.order_id:
            # ãƒ‘ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¤œè¨¼
            validate_path_components(self.order_id)

            staffing_file = safe_path_join(
                self.project_dir, "RESULT", self.order_id, "03_STAFFING.md"
            )
            if staffing_file.exists():
                return staffing_file.read_text(encoding="utf-8")

        # ã‚¿ã‚¹ã‚¯æƒ…å ±ã®ã¿ã§å®Ÿè¡Œ
        return f"ã‚¿ã‚¹ã‚¯: {self.task_info.get('title', 'Untitled')}"

    def _get_rework_history(self) -> tuple[int, str]:
        """
        REWORKå±¥æ­´ã‚’å–å¾—ï¼ˆREWORKå›žæ•°ã¨éŽåŽ»ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚³ãƒ¡ãƒ³ãƒˆï¼‰

        Returns:
            tuple[int, str]: (REWORKå›žæ•°, ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆæ¸ˆã¿REWORKå±¥æ­´ãƒ†ã‚­ã‚¹ãƒˆ)
        """
        try:
            conn = get_connection()
            try:
                # 1. reject_countã‹ã‚‰REWORKå›žæ•°ã‚’å–å¾—
                task = fetch_one(
                    conn,
                    "SELECT reject_count FROM tasks WHERE id = ? AND project_id = ?",
                    (self.task_id, self.project_id)
                )
                rework_count = task["reject_count"] if task else 0

                if rework_count == 0:
                    return (0, "")

                # 2. éŽåŽ»ã®REJECTEDåˆ¤å®šã¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’change_historyã‹ã‚‰å–å¾—
                # change_historyã«ã¯DONEâ†’REWORKã®é·ç§»ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹
                past_reviews = fetch_all(
                    conn,
                    """
                    SELECT change_reason as comment, changed_at as reviewed_at
                    FROM change_history
                    WHERE entity_type = 'task'
                      AND entity_id = ?
                      AND field_name = 'status'
                      AND new_value = 'REWORK'
                    ORDER BY changed_at DESC
                    """,
                    (self.task_id,)
                )

                if not past_reviews:
                    return (rework_count, "")

                # 3. ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ
                history_entries = []
                for idx, review in enumerate(rows_to_dicts(past_reviews), 1):
                    reviewed_at = review.get("reviewed_at", "ä¸æ˜Ž")
                    comment = review.get("comment", "ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆãªã—ï¼‰")
                    history_entries.append(f"""### REWORK #{idx} ({reviewed_at})
{comment}""")

                history_section = "\n\n".join(history_entries)

                return (rework_count, f"""
## ðŸ”„ REWORKå±¥æ­´ï¼ˆå¿…èª­ï¼‰

ã“ã®ã‚¿ã‚¹ã‚¯ã¯éŽåŽ»ã«{rework_count}å›žå·®ã—æˆ»ã•ã‚Œã¦ã„ã¾ã™ã€‚ä»¥ä¸‹ã®éŽåŽ»ã®æŒ‡æ‘˜äº‹é …ã‚’ç¢ºèªã—ã€åŒã˜å•é¡Œã‚’ç¹°ã‚Šè¿”ã•ãªã„ã‚ˆã†ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

{history_section}

""")
            finally:
                conn.close()

        except Exception as e:
            logger.warning(f"REWORKå±¥æ­´å–å¾—ã«å¤±æ•—: {e}")
            return (0, "")

    def _get_known_bugs(self) -> str:
        """æ—¢çŸ¥ã®ãƒã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’DBã‹ã‚‰å–å¾—ã—ã¦ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ"""
        try:
            conn = get_connection()
            try:
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå›ºæœ‰ + æ±Žç”¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—ï¼ˆACTIVEã®ã¿ï¼‰
                bugs = fetch_all(
                    conn,
                    """
                    SELECT id, title, description, pattern_type, severity, solution,
                           effectiveness_score
                    FROM bugs
                    WHERE (project_id = ? OR project_id IS NULL)
                      AND status = 'ACTIVE'
                    ORDER BY
                        severity DESC,
                        occurrence_count DESC
                    """,
                    (self.project_id,)
                )

                if not bugs:
                    return ""

                bugs_list = rows_to_dicts(bugs)

                # ãƒã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³æ³¨å…¥ã‚’è¨˜éŒ²ï¼ˆæœ‰åŠ¹æ€§è©•ä¾¡ç”¨ï¼‰
                try:
                    from quality.bug_learner import EffectivenessEvaluator
                    evaluator = EffectivenessEvaluator(self.project_id)
                    for bug in bugs_list:
                        try:
                            evaluator.record_injection(bug["id"])
                        except Exception:
                            pass  # å€‹åˆ¥ã®è¨˜éŒ²å¤±æ•—ã¯ç„¡è¦–
                except ImportError:
                    pass  # quality.bug_learner åˆ©ç”¨ä¸å¯æ™‚ã¯è¨˜éŒ²ã‚’ã‚¹ã‚­ãƒƒãƒ—
                except Exception:
                    pass  # è¨˜éŒ²å¤±æ•—ã¯å…ƒã®å‹•ä½œã«å½±éŸ¿ã•ã›ãªã„

                # ãƒã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ
                bug_entries = []
                for bug in bugs_list:
                    scope = "æ±Žç”¨" if bug.get("project_id") is None else "å›ºæœ‰"
                    pattern_label = f" [{bug['pattern_type']}]" if bug.get("pattern_type") else ""
                    eff_score = bug.get("effectiveness_score")
                    eff_label = f" [æœ‰åŠ¹æ€§: {eff_score:.2f}]" if eff_score is not None else ""

                    entry = f"""### {bug['id']}{pattern_label} - {bug['title']} ({scope}, {bug['severity']}){eff_label}
{bug['description']}"""

                    if bug.get("solution"):
                        entry += f"\n**è§£æ±ºç­–**: {bug['solution']}"

                    bug_entries.append(entry)

                bug_section = "\n\n".join(bug_entries)

                return f"""
## âš ï¸ æ—¢çŸ¥ãƒã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆå¿…èª­ï¼‰

ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŠã‚ˆã³ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å…¨ä½“ã§éŽåŽ»ã«ç™ºç”Ÿã—ãŸãƒã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚
å®Ÿè£…å‰ã«å¿…ãšç¢ºèªã—ã€åŒã˜ãƒŸã‚¹ã‚’ç¹°ã‚Šè¿”ã•ãªã„ã‚ˆã†ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚

{bug_section}

"""
            finally:
                conn.close()

        except Exception as e:
            logger.warning(f"æ—¢çŸ¥ãƒã‚°å–å¾—ã«å¤±æ•—: {e}")
            return ""

    def _build_execution_prompt(self, task_content: str) -> str:
        """ã‚¿ã‚¹ã‚¯å®Ÿè¡Œç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ§‹ç¯‰"""
        # ãƒªãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€å·®ã—æˆ»ã—ã‚³ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ 
        rework_section = ""
        if self.is_rework and self.rework_comment:
            rework_section = f"""
## ãƒªãƒ¯ãƒ¼ã‚¯æƒ…å ±ï¼ˆå·®ã—æˆ»ã—å¯¾å¿œï¼‰
ã“ã®ã‚¿ã‚¹ã‚¯ã¯ãƒ¬ãƒ“ãƒ¥ãƒ¼ã§å·®ã—æˆ»ã•ã‚ŒãŸãƒªãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚ä»¥ä¸‹ã®æŒ‡æ‘˜äº‹é …ã«å¯¾å¿œã—ã¦ãã ã•ã„ã€‚

### å·®ã—æˆ»ã—ã‚³ãƒ¡ãƒ³ãƒˆ
{self.rework_comment}

### å¯¾å¿œæ–¹é‡
1. ä¸Šè¨˜ã®å•é¡Œç‚¹ã‚’ç¢ºèªã—ã€è©²å½“ç®‡æ‰€ã‚’ç‰¹å®šã—ã¦ãã ã•ã„
2. ä¿®æ­£æŒ‡é‡ã«å¾“ã£ã¦ä¿®æ­£ã‚’è¡Œã£ã¦ãã ã•ã„
3. ä¿®æ­£å¾Œã€å•é¡ŒãŒè§£æ±ºã•ã‚ŒãŸã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„

"""

        # å‰å›žå¤±æ•—æ™‚ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿½åŠ ï¼ˆãƒªãƒˆãƒ©ã‚¤æ™‚ï¼‰
        failure_context_section = ""
        try:
            from retry.retry_handler import RetryHandler
            handler = RetryHandler(self.project_id, self.task_id)
            failure_context = handler.get_failure_context()
            if failure_context:
                failure_context_section = f"""
## ðŸ”„ Previous Failure Context (Auto-Retry)

This task previously failed. Review the following context from INCIDENTS table:

{failure_context}

### Retry Instructions:
1. Analyze the root cause and understand what went wrong
2. Implement fixes to address the specific failure
3. Verify the fix resolves the issue before completing
4. Document the changes made to prevent recurrence

"""
        except Exception as e:
            logger.debug(f"Failed to get failure context: {e}")
            pass

        # ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¿ã‚¹ã‚¯ã®å ´åˆã€å®‰å…¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’è¿½åŠ 
        migration_section = ""
        task_title = self.task_info.get("title", "").lower()
        task_desc = self.task_info.get("description", "").lower()

        migration_keywords = [
            "migration", "ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³", "schema", "ã‚¹ã‚­ãƒ¼ãƒž",
            "alter table", "drop table", "create table"
        ]

        is_migration_task = any(
            keyword in task_title or keyword in task_desc
            for keyword in migration_keywords
        )

        if is_migration_task:
            migration_section = """
## âš ï¸ ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®‰å…¨ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

ã“ã®ã‚¿ã‚¹ã‚¯ã¯ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚¹ã‚­ãƒ¼ãƒžã®å¤‰æ›´ã‚’å«ã‚€å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
å¿…ãšä»¥ä¸‹ã®å®‰å…¨æ©Ÿæ§‹ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„:

### å¿…é ˆäº‹é …
1. **MigrationRunnerã®ä½¿ç”¨**
   - `backend/utils/migration_base.py` ã® `MigrationRunner` ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨
   - ç›´æŽ¥SQLã‚’å®Ÿè¡Œã›ãšã€å¿…ãšMigrationRunnerã‚’çµŒç”±ã™ã‚‹ã“ã¨

2. **å®‰å…¨æ©Ÿèƒ½ã®æ´»ç”¨**
   - MigrationRunnerã¯è‡ªå‹•çš„ã«ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™:
     * ä»–ã®Workerå®Ÿè¡Œä¸­ã‚¿ã‚¹ã‚¯ã®æ¤œå‡ºã¨è­¦å‘Š
     * PRAGMA foreign_keys ã®è‡ªå‹•åˆ¶å¾¡ï¼ˆCASCADEå‰Šé™¤é˜²æ­¢ï¼‰
     * è‡ªå‹•ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
     * ãƒˆãƒ©ãƒ³ã‚¶ã‚¯ã‚·ãƒ§ãƒ³ç®¡ç†

3. **å®Ÿè£…ä¾‹**
   ```python
   from utils.migration_base import MigrationRunner, MigrationError

   def my_migration(conn):
       cursor = conn.cursor()
       cursor.execute("ALTER TABLE ...")
       return True

   runner = MigrationRunner("migration_name", verbose=True)
   success = runner.run(my_migration)
   ```

### ç¦æ­¢äº‹é …
- âŒ ç›´æŽ¥ `sqlite3.connect()` ã§DBæŽ¥ç¶šã—ãªã„
- âŒ ç›´æŽ¥ `DROP TABLE` ã‚„ `ALTER TABLE` ã‚’å®Ÿè¡Œã—ãªã„
- âŒ `PRAGMA foreign_keys = OFF` ã‚’æ‰‹å‹•ã§å®Ÿè¡Œã—ãªã„

MigrationRunnerã‚’ä½¿ç”¨ã—ãªã„å ´åˆã€CASCADEå‰Šé™¤ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿æå¤±ã®ãƒªã‚¹ã‚¯ãŒã‚ã‚Šã¾ã™ã€‚

"""

        # Workerç’°å¢ƒåˆ¶ç´„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå¸¸ã«æ³¨å…¥ï¼‰
        worker_env_section = """
## âš ï¸ Workerç’°å¢ƒåˆ¶ç´„ï¼ˆå¿…é ˆéµå®ˆï¼‰

Workerã¯ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ï¼ˆCLIï¼‰æ“ä½œã®ã¿å¯èƒ½ã§ã™ã€‚ä»¥ä¸‹ã®GUIæ“ä½œã¯**å®Ÿè¡Œä¸å¯èƒ½**ã§ã™:
- ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•ãƒ»ç”»é¢æ“ä½œ
- ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚·ãƒ§ãƒƒãƒˆæ’®å½±ãƒ»ç›®è¦–ç¢ºèª
- ãƒ–ãƒ©ã‚¦ã‚¶èµ·å‹•ãƒ»Webç”»é¢æ“ä½œ
- GUIãƒ†ã‚¹ãƒˆï¼ˆE2Eãƒ†ã‚¹ãƒˆç­‰ã®ç”»é¢æ“ä½œã‚’ä¼´ã†ã‚‚ã®ï¼‰

**å“è³ªç¢ºèªã®ä»£æ›¿æ‰‹æ®µ**: GUIæ“ä½œã®ä»£ã‚ã‚Šã«ä»¥ä¸‹ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„:
- `npm run build` ï¼ˆãƒ“ãƒ«ãƒ‰æˆåŠŸç¢ºèªï¼‰
- `tsc --noEmit` ï¼ˆåž‹ãƒã‚§ãƒƒã‚¯ï¼‰
- `npm test` ï¼ˆãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œï¼‰

GUIæ“ä½œã‚’å«ã‚€ã‚¿ã‚¹ã‚¯ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸå ´åˆã¯ã€ä¸Šè¨˜ã®ä»£æ›¿æ‰‹æ®µã§å“è³ªç¢ºèªã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

"""

        # Roamingãƒ‘ã‚¹ãƒ«ãƒ¼ãƒ«ï¼ˆBUG_011å¯¾ç­–: Localã¸ã®æ›¸ãè¾¼ã¿é˜²æ­¢ï¼‰
        roaming_path_section = f"""
## ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãƒ«ãƒ¼ãƒ«ï¼ˆPROJECTSé…ä¸‹ã¯Roamingçµ¶å¯¾ãƒ‘ã‚¹å¿…é ˆï¼‰

PROJECTS/é…ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿æ›¸ãã™ã‚‹éš›ã¯ä»¥ä¸‹ã®**Roamingçµ¶å¯¾ãƒ‘ã‚¹**ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
ç›¸å¯¾ãƒ‘ã‚¹ `PROJECTS/{self.project_id}/...` ã¯**ç¦æ­¢**ã§ã™ï¼ˆcwdãŒLocalã®ãŸã‚Localã«æ›¸ãè¾¼ã¾ã‚Œã¾ã™ï¼‰ã€‚

| ç”¨é€” | çµ¶å¯¾ãƒ‘ã‚¹ |
|------|---------|
| ãƒ™ãƒ¼ã‚¹ | `{self.project_dir}` |
| RESULT | `{self.project_dir / "RESULT"}` |
| ORDERS | `{self.project_dir / "ORDERS"}` |
| PROJECT_INFO.md | `{self.project_dir / "PROJECT_INFO.md"}` |

**ç†ç”±**: Squirrelã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ©ãƒ¼ã®æ›´æ–°ã§AppData\\LocalãŒä¸Šæ›¸ãã•ã‚Œã‚‹ãŸã‚ã€æ°¸ç¶šãƒ‡ãƒ¼ã‚¿ã¯å¿…ãšAppData\\Roamingé…ä¸‹ã«é…ç½®ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

"""

        # REWORKå±¥æ­´ã‚’å–å¾—
        rework_count, rework_history_section = self._get_rework_history()

        # æ—¢çŸ¥ãƒã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å–å¾—
        known_bugs_section = self._get_known_bugs()

        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ãƒ«ãƒ¼ãƒ«ã‚’è¿½åŠ 
        test_file_rules_section = """
## ðŸ“ ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ãƒ«ãƒ¼ãƒ«ï¼ˆå¿…èª­ï¼‰

**CRITICAL**: ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚„ãƒ‡ãƒãƒƒã‚°ç”¨ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¯å¿…ãš `tmp/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

### ãƒ«ãƒ¼ãƒ«
1. **å‡ºåŠ›å…ˆ**: `AI_PM/tmp/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆç›´ä¸‹ã®tmp/ï¼‰
2. **ç¦æ­¢**: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆç›´ä¸‹ã¸ã®ç›´æŽ¥ä½œæˆï¼ˆä¾‹: `test_*.py`, `tmp_*.json` ãªã©ï¼‰
3. **å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«**:
   - ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ`test_*.py`, `*_test.py`ï¼‰
   - ä¸€æ™‚JSONãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`tmp_*.json`, `temp_*.json`ï¼‰
   - ãƒ‡ãƒãƒƒã‚°ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`debug_*.txt`, `*.log`ï¼‰
   - ãã®ä»–ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«

### ç†ç”±
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆç›´ä¸‹ãŒæ•£ã‚‰ã‹ã‚‹ã®ã‚’é˜²ã
- Gitç®¡ç†å¯¾è±¡å¤–ã¨ã™ã‚‹ï¼ˆtmp/ã¯.gitignoreã«è¿½åŠ æ¸ˆã¿ï¼‰
- ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤ãƒ»ç®¡ç†ã‚’å®¹æ˜“ã«ã™ã‚‹

### ä¾‹
```python
# âŒ ç¦æ­¢
output_file = "test_output.json"
output_file = "tmp_results.json"

# âœ… æ­£ã—ã„
output_file = "tmp/test_output.json"
output_file = "tmp/tmp_results.json"
```

"""

        mode_label = "ã€ãƒªãƒ¯ãƒ¼ã‚¯ã€‘" if self.is_rework else ""

        # REWORKå›žæ•°ã«å¿œã˜ãŸè­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        rework_warning = ""
        if rework_count >= 2:
            rework_warning = f"""
âš ï¸ **é‡è¦**: ã“ã®ã‚¿ã‚¹ã‚¯ã¯{rework_count}å›žå·®ã—æˆ»ã•ã‚Œã¦ã„ã¾ã™ã€‚
- ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«: **{self.model.upper()}** (è‡ªå‹•æ˜‡æ ¼é©ç”¨æ¸ˆã¿)
- éŽåŽ»ã®æŒ‡æ‘˜äº‹é …ã‚’å¿…ãšç¢ºèªã—ã€åŒã˜å•é¡Œã‚’ç¹°ã‚Šè¿”ã•ãªã„ã§ãã ã•ã„
- ã‚ˆã‚Šæ…Žé‡ã«å®Ÿè£…ã—ã€ãƒ†ã‚¹ãƒˆã‚’å¾¹åº•ã—ã¦ãã ã•ã„

"""

        return f"""{mode_label}ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚
{rework_warning}
## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæƒ…å ±
- ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID: {self.project_id}
- ã‚¿ã‚¹ã‚¯ID: {self.task_id}
- ORDER ID: {self.order_id}

## ã‚¿ã‚¹ã‚¯æƒ…å ±
- ã‚¿ã‚¤ãƒˆãƒ«: {self.task_info.get('title', 'Untitled')}
- èª¬æ˜Ž: {self.task_info.get('description', 'ï¼ˆãªã—ï¼‰')}
- å„ªå…ˆåº¦: {self.task_info.get('priority', 'P1')}
{rework_section}{rework_history_section}{failure_context_section}{migration_section}{worker_env_section}{roaming_path_section}{test_file_rules_section}{known_bugs_section}
## ã‚¿ã‚¹ã‚¯å®šç¾©
{task_content}

## æŒ‡ç¤º
1. ã‚¿ã‚¹ã‚¯ã®å†…å®¹ã‚’ç†è§£ã—ã€å®Œäº†æ¡ä»¶ã‚’ç¢ºèªã—ã¦ãã ã•ã„
2. å¿…è¦ãªå®Ÿè£…ãƒ»ä½œæ¥­ã‚’è¡Œã£ã¦ãã ã•ã„
3. å®Œäº†ã—ãŸã‚‰ã€å®Ÿæ–½å†…å®¹ã¨çµæžœã‚’JSONå½¢å¼ã§å ±å‘Šã—ã¦ãã ã•ã„

## å‡ºåŠ›å½¢å¼
JSONå½¢å¼ã§ä»¥ä¸‹ã®æ§‹é€ ã‚’è¿”ã—ã¦ãã ã•ã„:
{{
  "completed": true/false,
  "summary": "å®Ÿæ–½å†…å®¹ã®è¦ç´„",
  "details": ["è©³ç´°1", "è©³ç´°2", ...],
  "artifacts": ["ä½œæˆ/æ›´æ–°ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹1", ...],
  "issues": ["ç™ºç”Ÿã—ãŸå•é¡ŒãŒã‚ã‚Œã°è¨˜è¼‰"]
}}

JSONã®ã¿ã‚’å‡ºåŠ›ã—ã€èª¬æ˜Žæ–‡ã¯å«ã‚ãªã„ã§ãã ã•ã„ã€‚"""

    def _step_create_report(self) -> None:
        """Step 4: REPORTã‚’ä½œæˆï¼ˆæ›¸ãè¾¼ã¿æ¤œè¨¼ä»˜ãï¼‰"""
        self._log_step("create_report", "start", "")

        if not self.order_id:
            self._log_step("create_report", "skip", "ORDER ID ãªã—")
            return

        # ãƒ‘ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¤œè¨¼ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹æ··å…¥é˜²æ­¢ï¼‰
        validate_path_components(self.order_id, self.task_id)

        # REPORTãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        report_dir = safe_path_join(
            self.project_dir, "RESULT", self.order_id, "05_REPORT"
        )
        report_dir.mkdir(parents=True, exist_ok=True)

        report_file = safe_path_join(
            report_dir, f"REPORT_{self.task_id.replace('TASK_', '')}.md"
        )

        # execution_resultã®å†…å®¹ãƒã‚§ãƒƒã‚¯
        exec_result = self.results.get("execution_result", "")
        if not exec_result or len(exec_result.strip()) < 20:
            raise WorkerExecutionError(
                f"REPORTä½œæˆä¸å¯: execution_resultãŒç©ºã¾ãŸã¯çŸ­ã™ãŽã¾ã™ "
                f"({len(exec_result.strip()) if exec_result else 0}æ–‡å­—). "
                f"Workerå®Ÿè¡ŒãŒæ­£å¸¸ã«å®Œäº†ã—ã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
            )

        # REPORTå†…å®¹ä½œæˆ
        report_content = self._format_report(exec_result)

        if len(report_content.strip()) < 100:
            raise WorkerExecutionError(
                f"REPORTå†…å®¹ãŒçŸ­ã™ãŽã¾ã™ ({len(report_content.strip())}æ–‡å­—): "
                f"REPORTç”Ÿæˆã«å•é¡ŒãŒã‚ã£ãŸå¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚"
            )

        # ãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ã¨æ¤œè¨¼
        report_file.write_text(report_content, encoding="utf-8")

        # æ›¸ãè¾¼ã¿å¾Œã®å­˜åœ¨ãƒ»ã‚µã‚¤ã‚ºæ¤œè¨¼
        if not report_file.exists():
            raise WorkerExecutionError(f"REPORTãƒ•ã‚¡ã‚¤ãƒ«ã®æ›¸ãè¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {report_file}")

        written_size = report_file.stat().st_size
        if written_size < 100:
            raise WorkerExecutionError(
                f"REPORTãƒ•ã‚¡ã‚¤ãƒ«ãŒç•°å¸¸ã«å°ã•ã„ã§ã™ ({written_size}ãƒã‚¤ãƒˆ): {report_file}"
            )

        self.results["report_file"] = str(report_file)
        self.results["report_size_bytes"] = written_size

        self._log_step("create_report", "success", f"{report_file} ({written_size}ãƒã‚¤ãƒˆ)")

    def _format_report(self, exec_result: str) -> str:
        """REPORTå†…å®¹ã‚’ãƒ•ã‚©ãƒ¼ãƒžãƒƒãƒˆ"""
        lines = [
            f"# {self.task_id} å®Œäº†å ±å‘Š",
            "",
            "## åŸºæœ¬æƒ…å ±",
            "",
            "| é …ç›® | å†…å®¹ |",
            "|------|------|",
            f"| ã‚¿ã‚¹ã‚¯ID | {self.task_id} |",
            f"| å®Ÿè¡Œæ—¥æ™‚ | {datetime.now().strftime('%Y-%m-%d %H:%M')} |",
            f"| æ‹…å½“ | {self.worker_id} |",
            f"| ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ | å®Œäº† |",
            "",
        ]

        # å®Ÿè¡Œçµæžœã‚’ãƒ‘ãƒ¼ã‚¹
        try:
            result_data = json.loads(exec_result)
            lines.append("## å®Ÿæ–½å†…å®¹")
            lines.append("")
            lines.append(result_data.get("summary", "ï¼ˆè¦ç´„ãªã—ï¼‰"))
            lines.append("")

            if result_data.get("details"):
                lines.append("### è©³ç´°")
                for detail in result_data["details"]:
                    lines.append(f"- {detail}")
                lines.append("")

            if result_data.get("artifacts"):
                lines.append("### æˆæžœç‰©")
                for artifact in result_data["artifacts"]:
                    lines.append(f"- `{artifact}`")
                lines.append("")

            if result_data.get("issues"):
                lines.append("### ç™ºç”Ÿã—ãŸå•é¡Œ")
                for issue in result_data["issues"]:
                    lines.append(f"- {issue}")
                lines.append("")

        except json.JSONDecodeError:
            # JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ãã®ã¾ã¾è¨˜è¼‰
            lines.append("## å®Ÿè¡Œçµæžœ")
            lines.append("")
            lines.append(exec_result)

        # è‡ªå·±æ¤œè¨¼çµæžœã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
        verification = self.results.get("verification")
        if verification:
            lines.append("## è‡ªå·±æ¤œè¨¼çµæžœ")
            lines.append("")
            lines.append("| é …ç›® | çµæžœ |")
            lines.append("|------|------|")
            final_status = "PASSED" if verification.get("final_success") else "FAILED"
            lines.append(f"| æ¤œè¨¼çµæžœ | {final_status} |")
            lines.append(f"| è©¦è¡Œå›žæ•° | {verification.get('total_iterations', 0)}å›ž |")
            lines.append(f"| è‡ªå·±ä¿®æ­£ | {verification.get('fix_attempts', 0)}å›žå®Ÿæ–½ |")
            lines.append("")

            # æœ€å¾Œã®æ¤œè¨¼è©³ç´°
            history = verification.get("history", [])
            if history:
                last_entry = history[-1]
                checks = last_entry.get("checks", [])
                skipped = last_entry.get("skipped", [])

                if checks or skipped:
                    lines.append("### æ¤œè¨¼è©³ç´°")
                    for check in checks:
                        status = "PASS" if check.get("passed") else "FAIL"
                        lines.append(f"- [{status}] {check.get('type', '?')}: `{check.get('command', '?')}`")
                        if not check.get("passed") and check.get("errors"):
                            for err in check["errors"][:3]:
                                lines.append(f"  - {err}")
                    for skip_name in skipped:
                        lines.append(f"- [SKIP] {skip_name}: ãƒ„ãƒ¼ãƒ«æœªæ¤œå‡º")
                    lines.append("")

        return "\n".join(lines)

    def _step_static_analysis(self) -> None:
        """Step 4.5: æˆæžœç‰©ã«å¯¾ã—ã¦é™çš„è§£æžã‚’è‡ªå‹•å®Ÿè¡Œ

        StaticAnalyzer + AutoFixer ã‚’ä½¿ç”¨ã—ã¦ã€Workeræˆæžœç‰©ã®å“è³ªãƒã‚§ãƒƒã‚¯ã‚’è¡Œã†ã€‚
        è§£æžçµæžœã¯REPORTã«è¿½è¨˜ã—ã€tasksãƒ†ãƒ¼ãƒ–ãƒ«ã®static_analysis_scoreã‚’æ›´æ–°ã™ã‚‹ã€‚
        è§£æžå¤±æ•—æ™‚ã‚‚ã‚¿ã‚¹ã‚¯å‡¦ç†ã¯ç¶šè¡Œã™ã‚‹ï¼ˆã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚¹ã‚­ãƒƒãƒ—ï¼‰ã€‚
        """
        self._log_step("static_analysis", "start", "")

        try:
            from quality.static_analyzer import StaticAnalyzer
            from quality.auto_fixer import AutoFixer
        except ImportError as e:
            self._log_step("static_analysis", "skip", f"quality module not available: {e}")
            return

        # æˆæžœç‰©ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
        artifact_files = self._get_artifact_files()
        if not artifact_files:
            self._log_step("static_analysis", "skip", "æˆæžœç‰©ãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
            self.results["static_analysis"] = {"score": 100, "skipped": True}
            return

        try:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’æŽ¨å®š
            project_root = str(_project_root)

            # 1. è‡ªå‹•ä¿®æ­£ã‚’å…ˆã«å®Ÿè¡Œ
            fixer = AutoFixer(project_root)
            fix_result = fixer.fix(artifact_files)
            self._log_step("static_analysis", "info",
                f"auto_fix: {fix_result.get('fixed_count', 0)} files fixed")

            # 2. é™çš„è§£æžã‚’å®Ÿè¡Œ
            analyzer = StaticAnalyzer(project_root)
            analysis_result = analyzer.analyze(artifact_files)
            score = analysis_result.get("score", 100)
            self._log_step("static_analysis", "info",
                f"score={score}, errors={len(analysis_result.get('errors', []))}, "
                f"warnings={len(analysis_result.get('warnings', []))}")

            # 3. çµæžœã‚’ä¿æŒ
            self.results["static_analysis"] = {
                "score": score,
                "errors": analysis_result.get("errors", []),
                "warnings": analysis_result.get("warnings", []),
                "tools_used": analysis_result.get("tools_used", []),
                "fix_result": fix_result,
            }

            # 4. REPORTã«è¿½è¨˜
            self._append_static_analysis_to_report(analysis_result, fix_result)

            # 5. DBã®static_analysis_scoreã‚’æ›´æ–°
            self._update_static_analysis_score(score)

            self._log_step("static_analysis", "success", f"score={score}")

        except Exception as e:
            self._log_step("static_analysis", "warning", f"è§£æžå¤±æ•—ï¼ˆç¶šè¡Œï¼‰: {e}")
            self.results["static_analysis"] = {"score": None, "error": str(e)}

    def _get_artifact_files(self) -> list:
        """Workerå®Ÿè¡Œçµæžœã‹ã‚‰æˆæžœç‰©ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        artifact_files = []

        # æ–¹æ³•1: execution_resultã®artifactsã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’å–å¾—
        exec_result = self.results.get("execution_result", "")
        try:
            result_data = json.loads(exec_result)
            artifacts = result_data.get("artifacts", [])
            for artifact in artifacts:
                artifact_path = Path(artifact)
                if artifact_path.exists() and artifact_path.is_file():
                    artifact_files.append(str(artifact_path))
        except (json.JSONDecodeError, TypeError):
            pass

        # æ–¹æ³•2: æˆæžœç‰©ãŒå–ã‚Œãªã‹ã£ãŸå ´åˆã€REPORTã‹ã‚‰æŽ¢ç´¢ã¯çœç•¥
        # ï¼ˆæ–¹æ³•1ã§ååˆ†ãªæƒ…å ±ãŒå¾—ã‚‰ã‚Œã‚‹ã¯ãšï¼‰

        return artifact_files

    def _append_static_analysis_to_report(
        self, analysis_result: dict, fix_result: dict
    ) -> None:
        """REPORTãƒ•ã‚¡ã‚¤ãƒ«ã«é™çš„è§£æžçµæžœã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’è¿½è¨˜"""
        report_file_str = self.results.get("report_file")
        if not report_file_str:
            return

        report_file = Path(report_file_str)
        if not report_file.exists():
            return

        lines = ["", "## é™çš„è§£æžçµæžœ", ""]
        score = analysis_result.get("score", 100)
        lines.append(f"### ã‚¹ã‚³ã‚¢: {score}/100")
        lines.append("")

        tools_used = analysis_result.get("tools_used", [])
        skipped = analysis_result.get("skipped_tools", [])
        if tools_used:
            lines.append(f"### ä½¿ç”¨ãƒ„ãƒ¼ãƒ«: {', '.join(tools_used)}")
            lines.append("")
        if skipped:
            lines.append(f"### ã‚¹ã‚­ãƒƒãƒ—ãƒ„ãƒ¼ãƒ«: {', '.join(skipped)}")
            lines.append("")

        # è‡ªå‹•ä¿®æ­£çµæžœ
        fixes = fix_result.get("fixes", [])
        if fixes:
            lines.append(f"### è‡ªå‹•ä¿®æ­£æ¸ˆã¿ ({len(fixes)}ä»¶)")
            lines.append("")
            lines.append("| ãƒ•ã‚¡ã‚¤ãƒ« | ãƒ„ãƒ¼ãƒ« | å†…å®¹ |")
            lines.append("|---------|--------|------|")
            for fix in fixes:
                lines.append(f"| `{fix.get('file', '')}` | {fix.get('tool', '')} | {fix.get('description', '')} |")
            lines.append("")

        # ã‚¨ãƒ©ãƒ¼
        errors = analysis_result.get("errors", [])
        if errors:
            lines.append(f"### ã‚¨ãƒ©ãƒ¼ ({len(errors)}ä»¶)")
            lines.append("")
            lines.append("| ãƒ•ã‚¡ã‚¤ãƒ« | è¡Œ | ãƒ„ãƒ¼ãƒ« | å†…å®¹ |")
            lines.append("|---------|-----|--------|------|")
            for err in errors[:20]:  # æœ€å¤§20ä»¶
                lines.append(
                    f"| `{err.get('file', '')}` | {err.get('line', '')} | "
                    f"{err.get('tool', '')} | {err.get('message', '')} |"
                )
            if len(errors) > 20:
                lines.append(f"| ... | ... | ... | ä»–{len(errors) - 20}ä»¶çœç•¥ |")
            lines.append("")

        # è­¦å‘Š
        warnings = analysis_result.get("warnings", [])
        if warnings:
            lines.append(f"### è­¦å‘Š ({len(warnings)}ä»¶)")
            lines.append("")
            lines.append("| ãƒ•ã‚¡ã‚¤ãƒ« | è¡Œ | ãƒ„ãƒ¼ãƒ« | å†…å®¹ |")
            lines.append("|---------|-----|--------|------|")
            for warn in warnings[:10]:  # æœ€å¤§10ä»¶
                lines.append(
                    f"| `{warn.get('file', '')}` | {warn.get('line', '')} | "
                    f"{warn.get('tool', '')} | {warn.get('message', '')} |"
                )
            if len(warnings) > 10:
                lines.append(f"| ... | ... | ... | ä»–{len(warnings) - 10}ä»¶çœç•¥ |")
            lines.append("")

        # REPORTãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜
        existing = report_file.read_text(encoding="utf-8")
        report_file.write_text(existing + "\n".join(lines), encoding="utf-8")

    def _update_static_analysis_score(self, score: int) -> None:
        """tasksãƒ†ãƒ¼ãƒ–ãƒ«ã®static_analysis_scoreã‚’æ›´æ–°"""
        try:
            conn = get_connection()
            conn.execute(
                "UPDATE tasks SET static_analysis_score = ? WHERE id = ? AND project_id = ?",
                (score, self.task_id, self.project_id)
            )
            conn.commit()
        except Exception as e:
            # ã‚«ãƒ©ãƒ æœªè¿½åŠ æ™‚ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ãƒ‡ã‚°ãƒ¬ãƒ¼ãƒ‰ï¼‰
            logger.warning(f"static_analysis_scoreæ›´æ–°ã‚¹ã‚­ãƒƒãƒ—: {e}")

    def _step_destructive_sql_check(self) -> None:
        """Step 4.6: æˆæžœç‰©ã«å¯¾ã—ã¦ç ´å£Šçš„SQLæ“ä½œã‚’æ¤œå‡º

        DestructiveSqlDetectorã‚’ä½¿ç”¨ã—ã¦ã€Workeræˆæžœç‰©ã«ç ´å£Šçš„ãªDBå¤‰æ›´ï¼ˆDROP TABLE,
        ALTER TABLE DROP COLUMNç­‰ï¼‰ãŒå«ã¾ã‚Œã¦ã„ãªã„ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã€‚
        æ¤œå‡ºçµæžœã¯REPORTã«è¿½è¨˜ã—ã€PMãƒ¬ãƒ“ãƒ¥ãƒ¼æ™‚ã«ç¢ºèªã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
        æ¤œå‡ºå¤±æ•—æ™‚ã‚‚ã‚¿ã‚¹ã‚¯å‡¦ç†ã¯ç¶šè¡Œã™ã‚‹ï¼ˆã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚¹ã‚­ãƒƒãƒ—ï¼‰ã€‚
        """
        self._log_step("destructive_sql_check", "start", "")

        try:
            from utils.sql_safety import DestructiveSqlDetector
        except ImportError as e:
            self._log_step("destructive_sql_check", "skip", f"sql_safety module not available: {e}")
            return

        # æˆæžœç‰©ãƒ•ã‚¡ã‚¤ãƒ«ãƒªã‚¹ãƒˆã‚’å–å¾—
        artifact_files = self._get_artifact_files()
        if not artifact_files:
            self._log_step("destructive_sql_check", "skip", "æˆæžœç‰©ãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
            self.results["destructive_sql_check"] = {"checked": False, "skipped": True}
            return

        try:
            # DestructiveSqlDetectoråˆæœŸåŒ–
            detector = DestructiveSqlDetector()

            # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
            all_results = []
            total_matches = 0
            critical_count = 0
            high_count = 0
            medium_count = 0

            for artifact_path in artifact_files:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’çµ¶å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
                if not Path(artifact_path).is_absolute():
                    artifact_path = _project_root / artifact_path

                scan_result = detector.scan_file(artifact_path)
                if scan_result.has_destructive_operations:
                    all_results.append(scan_result)
                    total_matches += len(scan_result.matches)
                    critical_count += scan_result.critical_count
                    high_count += scan_result.high_count
                    medium_count += scan_result.medium_count

            # çµæžœã‚’ä¿æŒ
            self.results["destructive_sql_check"] = {
                "checked": True,
                "has_destructive_operations": len(all_results) > 0,
                "total_files_scanned": len(artifact_files),
                "files_with_issues": len(all_results),
                "total_matches": total_matches,
                "critical_count": critical_count,
                "high_count": high_count,
                "medium_count": medium_count,
                "results": [r.to_dict() for r in all_results],
            }

            if len(all_results) > 0:
                self._log_step(
                    "destructive_sql_check",
                    "warning",
                    f"ç ´å£Šçš„SQLæ¤œå‡º: {total_matches}ä»¶ (CRITICAL:{critical_count}, HIGH:{high_count}, MEDIUM:{medium_count})"
                )
            else:
                self._log_step("destructive_sql_check", "success", "ç ´å£Šçš„SQLæ“ä½œãªã—")

            # REPORTã«è¿½è¨˜
            self._append_destructive_sql_to_report(self.results["destructive_sql_check"])

        except Exception as e:
            # æ¤œå‡ºã‚¨ãƒ©ãƒ¼æ™‚ã‚‚ã‚¿ã‚¹ã‚¯å‡¦ç†ã¯ç¶šè¡Œ
            self._log_step("destructive_sql_check", "error", f"ç ´å£Šçš„SQLæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
            self.results["destructive_sql_check"] = {
                "checked": False,
                "error": str(e),
            }

    def _append_destructive_sql_to_report(self, check_result: Dict[str, Any]) -> None:
        """ç ´å£Šçš„SQLæ¤œå‡ºçµæžœã‚’REPORTã«è¿½è¨˜"""
        if not self.order_id:
            return

        # ãƒ‘ã‚¹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ¤œè¨¼ï¼ˆçµ¶å¯¾ãƒ‘ã‚¹æ··å…¥é˜²æ­¢ï¼‰
        validate_path_components(self.order_id, self.task_id)

        report_dir = safe_path_join(
            self.project_dir, "RESULT", self.order_id, "05_REPORT"
        )
        report_file = safe_path_join(
            report_dir, f"REPORT_{self.task_id.replace('TASK_', '')}.md"
        )

        if not report_file.exists():
            return

        lines = [
            "",
            "## ç ´å£Šçš„SQLæ¤œå‡ºçµæžœ",
            "",
        ]

        if not check_result.get("checked"):
            lines.append("âš ï¸ ç ´å£Šçš„SQLæ¤œå‡ºã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
            if check_result.get("error"):
                lines.append(f"ã‚¨ãƒ©ãƒ¼: `{check_result['error']}`")
            lines.append("")
        elif not check_result.get("has_destructive_operations"):
            lines.append("âœ… ç ´å£Šçš„SQLæ“ä½œã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            lines.append("")
        else:
            # æ¤œå‡ºã‚ã‚Š - è­¦å‘Šè¡¨ç¤º
            critical = check_result.get("critical_count", 0)
            high = check_result.get("high_count", 0)
            medium = check_result.get("medium_count", 0)
            total = check_result.get("total_matches", 0)

            lines.append(f"âš ï¸ **ç ´å£Šçš„SQLæ“ä½œãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸ** ({total}ä»¶)")
            lines.append("")
            lines.append(f"- CRITICAL: {critical}ä»¶")
            lines.append(f"- HIGH: {high}ä»¶")
            lines.append(f"- MEDIUM: {medium}ä»¶")
            lines.append("")

            # è©³ç´°ãƒªã‚¹ãƒˆ
            lines.append("### æ¤œå‡ºè©³ç´°")
            lines.append("")
            lines.append("| ãƒ•ã‚¡ã‚¤ãƒ« | è¡Œ | é‡è¦åº¦ | èª¬æ˜Ž | ã‚³ãƒ¼ãƒ‰ |")
            lines.append("|---------|-------|--------|------|--------|")

            for result in check_result.get("results", []):
                file_path = result.get("file_path", "")
                # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ã«å¤‰æ›
                try:
                    file_path = str(Path(file_path).relative_to(_project_root))
                except ValueError:
                    pass

                for match in result.get("matches", []):
                    severity = match.get("severity", "")
                    line_num = match.get("line_number", "")
                    desc = match.get("description", "")
                    code = match.get("line_content", "").strip()
                    # ã‚³ãƒ¼ãƒ‰éƒ¨åˆ†ã‚’çŸ­ç¸®ï¼ˆé•·ã™ãŽã‚‹å ´åˆï¼‰
                    if len(code) > 60:
                        code = code[:57] + "..."

                    severity_icon = {
                        "CRITICAL": "ðŸ”´",
                        "HIGH": "ðŸŸ ",
                        "MEDIUM": "ðŸŸ¡",
                    }.get(severity, "")

                    lines.append(
                        f"| `{file_path}` | {line_num} | {severity_icon} {severity} | {desc} | `{code}` |"
                    )

            lines.append("")
            lines.append("âš ï¸ **PMç¢ºèªäº‹é …**: ã“ã®ã‚¿ã‚¹ã‚¯ã«ã¯ç ´å£Šçš„ãªDBå¤‰æ›´ãŒå«ã¾ã‚Œã¾ã™ã€‚")
            lines.append("ãƒžã‚¤ã‚°ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œã‚¿ã‚¤ãƒŸãƒ³ã‚°ã¨å½±éŸ¿ç¯„å›²ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            lines.append("")

        # REPORTãƒ•ã‚¡ã‚¤ãƒ«ã«è¿½è¨˜
        existing = report_file.read_text(encoding="utf-8")
        report_file.write_text(existing + "\n".join(lines), encoding="utf-8")

    def _step_update_status_done(self) -> None:
        """Step 6: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’DONEã«æ›´æ–°"""
        self._log_step("update_status_done", "start", "")

        from task.update import update_task

        try:
            update_task(
                self.project_id,
                self.task_id,
                status="DONE",
                role="Worker",
            )
            self._log_step("update_status_done", "success", "status=DONE")

            # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ã‚’è§£æ”¾
            try:
                from utils.file_lock import FileLockManager
                FileLockManager.release_locks(self.project_id, self.task_id)
                self._log_step("file_lock_release", "success", "ãƒ­ãƒƒã‚¯è§£æ”¾å®Œäº†")

                # NOTE: ãƒ­ãƒƒã‚¯è§£æ”¾å¾Œã® auto_kick ã¯å‰Šé™¤ï¼ˆBACKLOG_167ä¿®æ­£ï¼‰
                # DONEã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã§ã¯å¾Œç¶šã‚¿ã‚¹ã‚¯ã‚’è§£é™¤ã—ãªã„ã€‚
                # ãƒ¬ãƒ“ãƒ¥ãƒ¼æ‰¿èªï¼ˆCOMPLETEDï¼‰å¾Œã« _step_check_successor_tasks() ã§è§£é™¤ã™ã‚‹ã€‚

            except ImportError:
                self._log_step("file_lock_release", "skip", "FileLockManageråˆ©ç”¨ä¸å¯")
            except Exception as e:
                # ãƒ­ãƒƒã‚¯è§£æ”¾å¤±æ•—ã¯è­¦å‘Šã®ã¿ï¼ˆã‚¿ã‚¹ã‚¯ã¯å®Œäº†ã—ã¦ã„ã‚‹ï¼‰
                self._log_step("file_lock_release", "warning", f"ãƒ­ãƒƒã‚¯è§£æ”¾ã‚¨ãƒ©ãƒ¼: {e}")

        except Exception as e:
            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æ›´æ–°å¤±æ•—ã¯è­¦å‘Šã®ã¿
            self._log_step("update_status_done", "warning", str(e))

        # è‡ªå·±æ¤œè¨¼çµæžœã‚’change_historyã«è¨˜éŒ²
        self._record_verification_history()

    def _record_verification_history(self) -> None:
        """è‡ªå·±æ¤œè¨¼çµæžœã‚’change_historyãƒ†ãƒ¼ãƒ–ãƒ«ã«è¨˜éŒ²"""
        verification = self.results.get("verification")
        if not verification:
            return

        try:
            conn = get_connection()
            try:
                description = json.dumps({
                    "final_success": verification.get("final_success", False),
                    "total_iterations": verification.get("total_iterations", 0),
                    "fix_attempts": verification.get("fix_attempts", 0),
                }, ensure_ascii=False)

                execute_query(
                    conn,
                    """
                    INSERT INTO change_history
                        (project_id, entity_type, entity_id, change_type, description, changed_by, changed_at)
                    VALUES (?, 'task', ?, 'self_verification', ?, 'Worker', ?)
                    """,
                    (self.project_id, self.task_id, description, datetime.now().isoformat())
                )
                conn.commit()
                self._log_step("verification_history", "success", "change_historyã«è¨˜éŒ²")
            finally:
                conn.close()
        except Exception as e:
            # è¨˜éŒ²å¤±æ•—ã¯è­¦å‘Šã®ã¿ï¼ˆã‚¿ã‚¹ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
            self._log_step("verification_history", "warning", f"è¨˜éŒ²å¤±æ•—: {e}")

    def _get_next_queued_task(self) -> Optional[str]:
        """
        åŒä¸€ORDERå†…ã®æ¬¡ã®å®Ÿè¡Œå¯èƒ½ã‚¿ã‚¹ã‚¯ã‚’å–å¾—
        QUEUEDçŠ¶æ…‹ã¨REWORKçŠ¶æ…‹ã®ã‚¿ã‚¹ã‚¯ã‚’å¯¾è±¡ã¨ã™ã‚‹
        ä¾å­˜é–¢ä¿‚ã‚’è€ƒæ…®ã—ã€ä¾å­˜ã‚¿ã‚¹ã‚¯ãŒã™ã¹ã¦COMPLETEDã®ã‚¿ã‚¹ã‚¯ã®ã¿ã‚’è¿”ã™

        Returns:
            æ¬¡ã®ã‚¿ã‚¹ã‚¯IDã€ãªã‘ã‚Œã°None
        """
        if not self.order_id:
            return None

        conn = get_connection()
        try:
            # åŒä¸€ORDERå†…ã®QUEUED/REWORKã‚¿ã‚¹ã‚¯ã‚’å„ªå…ˆåº¦é †ã€ä½œæˆæ—¥æ™‚é †ã§å–å¾—
            # BLOCKEDã‚¿ã‚¹ã‚¯ã¯é™¤å¤–ï¼ˆä¾å­˜é–¢ä¿‚ä¿è­·ï¼‰
            # REWORKã‚¿ã‚¹ã‚¯ã‚’å„ªå…ˆï¼ˆå·®ã—æˆ»ã—å¯¾å¿œã‚’å…ˆã«å‡¦ç†ï¼‰
            tasks = fetch_all(
                conn,
                """
                SELECT id, status FROM tasks
                WHERE project_id = ?
                  AND order_id = ?
                  AND status IN ('QUEUED', 'REWORK')
                ORDER BY
                    CASE status
                        WHEN 'REWORK' THEN 0
                        WHEN 'QUEUED' THEN 1
                        ELSE 2
                    END,
                    CASE priority
                        WHEN 'P0' THEN 0
                        WHEN 'P1' THEN 1
                        WHEN 'P2' THEN 2
                        ELSE 3
                    END,
                    created_at ASC
                """,
                (self.project_id, self.order_id)
            )

            # å„ã‚¿ã‚¹ã‚¯ã®ä¾å­˜é–¢ä¿‚ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å®Ÿè¡Œå¯èƒ½ãªã‚¿ã‚¹ã‚¯ã‚’è¿”ã™
            for task in tasks:
                task_id = task["id"]
                if self._is_task_ready_to_execute(conn, task_id):
                    self._log_step("next_task_check", "info", f"æ¬¡ã‚¿ã‚¹ã‚¯é¸å®š: {task_id} (ä¾å­˜é–¢ä¿‚ã‚¯ãƒªã‚¢)")
                    return task_id
                else:
                    self._log_step("next_task_check", "debug", f"ã‚¿ã‚¹ã‚¯ {task_id} ã¯ä¾å­˜å¾…ã¡")

            return None

        finally:
            conn.close()

    def _is_task_ready_to_execute(self, conn, task_id: str) -> bool:
        """
        ã‚¿ã‚¹ã‚¯ãŒå®Ÿè¡Œå¯èƒ½ã‹ã©ã†ã‹ã‚’ãƒã‚§ãƒƒã‚¯
        ä¾å­˜ã‚¿ã‚¹ã‚¯ãŒã™ã¹ã¦COMPLETEDã§ã‚ã‚Šã€ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ç«¶åˆãŒãªã‘ã‚Œã°True

        Args:
            conn: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æŽ¥ç¶š
            task_id: ãƒã‚§ãƒƒã‚¯å¯¾è±¡ã®ã‚¿ã‚¹ã‚¯ID

        Returns:
            å®Ÿè¡Œå¯èƒ½ãªã‚‰Trueã€ä¾å­˜å¾…ã¡ã¾ãŸã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ç«¶åˆãªã‚‰False
        """
        # ä¾å­˜ã‚¿ã‚¹ã‚¯ã®ã†ã¡ã€ã¾ã COMPLETEDã§ãªã„ã‚‚ã®ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
        pending_deps = fetch_one(
            conn,
            """
            SELECT COUNT(*) as count
            FROM task_dependencies td
            JOIN tasks t ON td.depends_on_task_id = t.id AND td.project_id = t.project_id
            WHERE td.task_id = ? AND td.project_id = ?
            AND t.status != 'COMPLETED'
            """,
            (task_id, self.project_id)
        )

        # æœªå®Œäº†ã®ä¾å­˜ãŒã‚ã‚Œã°å®Ÿè¡Œä¸å¯
        if pending_deps and pending_deps["count"] > 0:
            return False

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ç«¶åˆã‚’ãƒã‚§ãƒƒã‚¯
        try:
            from utils.file_lock import FileLockManager
            can_start, blocking_tasks = FileLockManager.can_task_start(self.project_id, task_id)

            if not can_start:
                self._log_step(
                    "file_lock_check",
                    "debug",
                    f"ã‚¿ã‚¹ã‚¯ {task_id} ã¯ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯ç«¶åˆï¼ˆãƒ–ãƒ­ãƒƒã‚¯å…ƒ: {', '.join(blocking_tasks)}ï¼‰"
                )
                return False

        except ImportError:
            # FileLockManagerãŒãªã„å ´åˆã¯ãƒ­ãƒƒã‚¯ãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
            pass
        except Exception as e:
            # ãƒ­ãƒƒã‚¯ãƒã‚§ãƒƒã‚¯å¤±æ•—ã¯è­¦å‘Šã®ã¿
            self._log_step("file_lock_check", "warning", f"ãƒ­ãƒƒã‚¯ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")

        return True

    def _auto_kick_unblocked_tasks(self) -> None:
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯è§£æ”¾å¾Œã€å¾…æ©Ÿã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•å†è©•ä¾¡ã—ã¦ã‚­ãƒƒã‚¯

        ã‚¿ã‚¹ã‚¯å®Œäº†æ™‚ã«ã€åŒä¸€ORDERå†…ã®BLOCKED/QUEUEDã‚¿ã‚¹ã‚¯ã®ä¾å­˜é–¢ä¿‚ã¨
        ãƒ•ã‚¡ã‚¤ãƒ«ãƒ­ãƒƒã‚¯çŠ¶æ…‹ã‚’å†è©•ä¾¡ã—ã€å®Ÿè¡Œå¯èƒ½ã«ãªã£ãŸã‚¿ã‚¹ã‚¯ã‚’
        BLOCKED â†’ QUEUED ã«è‡ªå‹•æ›´æ–°ã™ã‚‹ã€‚
        """
        if not self.order_id:
            self._log_step("auto_kick", "skip", "ORDER ID ãªã—")
            return

        self._log_step("auto_kick", "start", f"order={self.order_id}")

        try:
            from utils.task_unblock import TaskUnblocker

            # å¾…æ©Ÿã‚¿ã‚¹ã‚¯ã‚’æ¤œå‡ºã—ã¦ã‚­ãƒƒã‚¯
            kicked_tasks = TaskUnblocker.auto_kick_unblocked_tasks(
                self.project_id,
                self.order_id,
                exclude_task_id=self.task_id,
                max_kicks=10  # ä¸€åº¦ã«æœ€å¤§10ã‚¿ã‚¹ã‚¯ã¾ã§ã‚­ãƒƒã‚¯
            )

            if kicked_tasks:
                task_ids = [t["id"] for t in kicked_tasks]
                self.results["kicked_tasks"] = task_ids
                self._log_step(
                    "auto_kick",
                    "success",
                    f"{len(kicked_tasks)}ã‚¿ã‚¹ã‚¯ã‚’ã‚­ãƒƒã‚¯: {', '.join(task_ids)}"
                )
            else:
                self._log_step("auto_kick", "info", "ã‚­ãƒƒã‚¯å¯èƒ½ãªã‚¿ã‚¹ã‚¯ãªã—")

        except ImportError:
            self._log_step("auto_kick", "skip", "TaskUnblockeråˆ©ç”¨ä¸å¯")
        except Exception as e:
            # ã‚­ãƒƒã‚¯å¤±æ•—ã¯è­¦å‘Šã®ã¿ï¼ˆã‚¿ã‚¹ã‚¯å®Œäº†ã¯æˆåŠŸã—ã¦ã„ã‚‹ï¼‰
            self._log_step("auto_kick", "warning", f"è‡ªå‹•ã‚­ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            if self.verbose:
                logger.exception("auto_kickè©³ç´°ã‚¨ãƒ©ãƒ¼")

    def _step_auto_review(self) -> Dict[str, Any]:
        """Step 7: ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è‡ªå‹•å®Ÿè¡Œ"""
        self._log_step("auto_review", "start", f"model={self.review_model}")

        try:
            # ReviewProcessorã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
            from review.process_review import ReviewProcessor

            # ãƒ¬ãƒ“ãƒ¥ãƒ¼å‡¦ç†ã‚’å®Ÿè¡Œ
            processor = ReviewProcessor(
                self.project_id,
                self.task_id,
                dry_run=self.dry_run,
                skip_ai=self.skip_ai,
                auto_approve=False,  # AIãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å®Ÿè¡Œ
                verbose=self.verbose,
                timeout=self.timeout,
                model=self.review_model,
            )

            result = processor.process()
            return result

        except ImportError as e:
            self._log_step("auto_review", "error", f"ReviewProcessorã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}")
            return {"success": False, "error": f"ReviewProcessorã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•—: {e}"}
        except Exception as e:
            self._log_step("auto_review", "error", f"ãƒ¬ãƒ“ãƒ¥ãƒ¼å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
            return {"success": False, "error": str(e)}

    def _step_record_bug_fix(self) -> None:
        """Step 6.5: ãƒã‚°ä¿®æ­£ã‚¿ã‚¹ã‚¯ã®è‡ªå‹•è¨˜éŒ²ï¼ˆORDER_007ï¼‰

        ã‚¿ã‚¹ã‚¯ã‚¿ã‚¤ãƒˆãƒ«ã«ãƒã‚°ä¿®æ­£ã‚’ç¤ºã™ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã‚‹å ´åˆã€
        record_fix.pyã‚’å‘¼ã³å‡ºã—ã¦DB+PROJECT_INFO.mdã«è¨˜éŒ²ã‚’ä¿ƒã™ã€‚
        """
        try:
            task_title = self.task_info.get("title", "") if self.task_info else ""

            # ãƒã‚°ä¿®æ­£ã‚¿ã‚¹ã‚¯ã‹ã©ã†ã‹ã‚’åˆ¤å®š
            BUG_FIX_KEYWORDS = [
                "ãƒã‚°ä¿®æ­£", "ãƒã‚°å¯¾å¿œ", "bug fix", "bugfix", "hotfix",
                "ä¸å…·åˆä¿®æ­£", "éšœå®³å¯¾å¿œ", "ã‚¨ãƒ©ãƒ¼ä¿®æ­£", "ä¿®æ­£å¯¾å¿œ",
            ]
            is_bug_fix = any(kw.lower() in task_title.lower() for kw in BUG_FIX_KEYWORDS)

            if not is_bug_fix:
                self._log_step("record_bug_fix", "skip", "ãƒã‚°ä¿®æ­£ã‚¿ã‚¹ã‚¯ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
                return

            # REPORTã‹ã‚‰ãƒã‚°æƒ…å ±ã‚’æŠ½å‡º
            report_content = self.results.get("report_content", "")
            if not report_content:
                self._log_step("record_bug_fix", "skip", "REPORTå†…å®¹ãªã—")
                return

            from bugs.record_fix import record_fix

            result = record_fix(
                project_id=self.project_id,
                title=task_title,
                description=f"ã‚¿ã‚¹ã‚¯ {self.task_id} ã§ä¿®æ­£ã•ã‚ŒãŸãƒã‚°",
                solution=f"è©³ç´°ã¯REPORTã‚’å‚ç…§: REPORT_{self.task_id.replace('TASK_', '')}.md",
                severity="Medium",
                task_id=self.task_id,
                order_id=self.task_info.get("order_id") if self.task_info else None,
            )

            if result.success:
                self._log_step("record_bug_fix", "success", result.message)
                self.results["bug_fix_record"] = {
                    "bug_id": result.bug_id,
                    "rule_id": result.rule_id,
                    "bug_history_id": result.bug_history_id,
                }
            else:
                self._log_step("record_bug_fix", "warning", f"è¨˜éŒ²å¤±æ•—: {result.error}")

        except ImportError:
            self._log_step("record_bug_fix", "skip", "bugs.record_fix åˆ©ç”¨ä¸å¯")
        except Exception as e:
            # ãƒã‚°è¨˜éŒ²å¤±æ•—ã¯ãƒ¯ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã¿ï¼ˆãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
            self._log_step("record_bug_fix", "warning", f"ãƒã‚°è¨˜éŒ²ã‚¨ãƒ©ãƒ¼: {e}")

    def _step_bug_learning(self, review_result: Dict[str, Any]) -> None:
        """Step 7.5: ãƒã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³è‡ªå‹•å­¦ç¿’

        ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæžœã«å¿œã˜ã¦ãƒã‚°å­¦ç¿’ã‚’å®Ÿè¡Œ:
        - APPROVE: EffectivenessEvaluator.evaluate_all() ã‚’å‘¼ã³å‡ºã—ï¼ˆä½Žé »åº¦ã§å®Ÿè¡Œï¼‰
        - REJECT: BugLearner.learn_from_failure() ã‚’å‘¼ã³å‡ºã—
        """
        try:
            from quality.bug_learner import BugLearner, EffectivenessEvaluator

            verdict = review_result.get("verdict", "")

            if verdict == "REJECT":
                # å·®ã—æˆ»ã—æ™‚: ãƒã‚°ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’ã‚’å®Ÿè¡Œ
                learner = BugLearner(self.project_id)
                comment = review_result.get("comment", review_result.get("review_comment", ""))
                task_title = self.task_info.get("title", "")

                learn_result = learner.learn_from_failure(
                    self.task_id, comment, task_title
                )

                self.results["bug_learning"] = learn_result
                action = learn_result.get("action_taken", "unknown")
                self._log_step("bug_learning", "info", f"action={action}")

                if learn_result.get("matched_patterns"):
                    best = learn_result["matched_patterns"][0]
                    self._log_step(
                        "bug_learning", "info",
                        f"æ—¢å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒžãƒƒãƒ: {best['bug_id']} (similarity={best['similarity']})"
                    )
                elif learn_result.get("new_pattern_proposal"):
                    proposal = learn_result["new_pattern_proposal"]
                    self._log_step(
                        "bug_learning", "info",
                        f"æ–°è¦ãƒ‘ã‚¿ãƒ¼ãƒ³ææ¡ˆ: {proposal.get('proposed_id')} - {proposal.get('title')}"
                    )

                    # ORDER_007: å·®ã—æˆ»ã—2å›žä»¥ä¸Šã§è‡ªå‹•ç™»éŒ²
                    reject_count = self.task_info.get("reject_count", 0) if self.task_info else 0
                    if reject_count >= 2:
                        try:
                            from bugs.record_fix import record_fix
                            auto_result = record_fix(
                                project_id=self.project_id,
                                title=proposal.get("title", f"è‡ªå‹•æ¤œå‡ºãƒ‘ã‚¿ãƒ¼ãƒ³ - {self.task_id}"),
                                description=proposal.get("description", comment),
                                solution=proposal.get("solution", "å·®ã—æˆ»ã—ã‚³ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§"),
                                severity=proposal.get("severity", "Medium"),
                                pattern_type=proposal.get("cause_category"),
                                task_id=self.task_id,
                                skip_file=True,  # è‡ªå‹•ç™»éŒ²æ™‚ã¯DB ã®ã¿
                            )
                            if auto_result.success:
                                self._log_step(
                                    "bug_learning", "success",
                                    f"å·®ã—æˆ»ã—{reject_count}å›žâ†’è‡ªå‹•ç™»éŒ²: {auto_result.bug_id}"
                                )
                        except Exception as auto_err:
                            self._log_step("bug_learning", "warning", f"è‡ªå‹•ç™»éŒ²å¤±æ•—: {auto_err}")

            elif verdict == "APPROVE":
                # æ‰¿èªæ™‚: æœ‰åŠ¹æ€§è©•ä¾¡ã‚’å®Ÿè¡Œï¼ˆ10ã‚¿ã‚¹ã‚¯ã”ã¨ã«å®Ÿè¡Œ = ã‚¿ã‚¹ã‚¯IDã®æœ«å°¾0åˆ¤å®šï¼‰
                task_num = int(self.task_id.replace("TASK_", "")) if self.task_id.startswith("TASK_") else 0
                if task_num % 10 == 0:
                    evaluator = EffectivenessEvaluator(self.project_id)
                    eval_results = evaluator.evaluate_all()
                    deactivated = evaluator.deactivate_low_effectiveness()

                    self._log_step(
                        "bug_learning", "info",
                        f"æœ‰åŠ¹æ€§è©•ä¾¡å®Œäº†: {len(eval_results)}ãƒ‘ã‚¿ãƒ¼ãƒ³, éžã‚¢ã‚¯ãƒ†ã‚£ãƒ–åŒ–: {len(deactivated)}ä»¶"
                    )

        except ImportError:
            self._log_step("bug_learning", "skip", "quality.bug_learner åˆ©ç”¨ä¸å¯")
        except Exception as e:
            # ãƒã‚°å­¦ç¿’å¤±æ•—ã¯è­¦å‘Šã®ã¿ï¼ˆãƒ¡ã‚¤ãƒ³ãƒ•ãƒ­ãƒ¼ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„ï¼‰
            self._log_step("bug_learning", "warning", f"ãƒã‚°å­¦ç¿’ã‚¨ãƒ©ãƒ¼: {e}")

    def _step_check_successor_tasks(self) -> None:
        """Step 8: å¾Œç¶šã‚¿ã‚¹ã‚¯ã®ä¾å­˜é–¢ä¿‚ãƒã‚§ãƒƒã‚¯ã¨è‡ªå‹•èµ·å‹•"""
        self._log_step("check_successors", "start", f"task={self.task_id}")

        try:
            from utils.task_unblock import TaskUnblocker

            # å¾Œç¶šã‚¿ã‚¹ã‚¯ã‚’æ¤œå‡ºã—ã€å®Ÿè¡Œå¯èƒ½ãªã‚¿ã‚¹ã‚¯ã‚’ç‰¹å®š
            ready_tasks = TaskUnblocker.check_successor_dependencies(
                self.project_id,
                self.task_id
            )

            if not ready_tasks:
                self._log_step("check_successors", "info", "å®Ÿè¡Œå¯èƒ½ãªå¾Œç¶šã‚¿ã‚¹ã‚¯ãªã—")
                return

            # å®Ÿè¡Œå¯èƒ½ãªå¾Œç¶šã‚¿ã‚¹ã‚¯ã‚’BLOCKED â†’ QUEUEDã«æ›´æ–°
            kicked_successors = []
            for task in ready_tasks:
                task_id = task["id"]
                updated, new_status = TaskUnblocker.update_task_status_if_unblocked(
                    self.project_id,
                    task_id
                )

                if updated:
                    kicked_successors.append(task_id)
                    self._log_step(
                        "successor_kick",
                        "success",
                        f"{task_id}: {task.get('status')} â†’ {new_status}"
                    )

            if kicked_successors:
                self.results["kicked_successors"] = kicked_successors
                self._log_step(
                    "check_successors",
                    "success",
                    f"{len(kicked_successors)}ã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•èµ·å‹•: {', '.join(kicked_successors)}"
                )
            else:
                self._log_step("check_successors", "info", "å¾Œç¶šã‚¿ã‚¹ã‚¯ã¯æ—¢ã«QUEUED/å®Ÿè¡Œä¸­")

        except ImportError:
            self._log_step("check_successors", "skip", "TaskUnblockeråˆ©ç”¨ä¸å¯")
        except Exception as e:
            # å¾Œç¶šã‚¿ã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯å¤±æ•—ã¯è­¦å‘Šã®ã¿ï¼ˆç¾åœ¨ã®ã‚¿ã‚¹ã‚¯ã¯å®Œäº†ã—ã¦ã„ã‚‹ï¼‰
            self._log_step("check_successors", "warning", f"å¾Œç¶šã‚¿ã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            if self.verbose:
                logger.exception("check_successorsè©³ç´°ã‚¨ãƒ©ãƒ¼")


def main():
    """CLI ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    # Windowsç’°å¢ƒã§ã®UTF-8å‡ºåŠ›è¨­å®š
    try:
        from config import setup_utf8_output
        setup_utf8_output()
    except ImportError:
        pass

    parser = argparse.ArgumentParser(
        description="Workerå‡¦ç†ã‚’1ã‚³ãƒžãƒ³ãƒ‰ã§å®Ÿè¡Œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )

    parser.add_argument("project_id", help="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆID")
    parser.add_argument("task_id", help="ã‚¿ã‚¹ã‚¯IDï¼ˆä¾‹: 602 ã¾ãŸã¯ TASK_602ï¼‰")
    parser.add_argument("--dry-run", action="store_true", help="å®Ÿè¡Œè¨ˆç”»ã®ã¿è¡¨ç¤º")
    parser.add_argument("--skip-ai", action="store_true", help="AIå‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—")
    parser.add_argument("--verbose", "-v", action="store_true", help="è©³ç´°ãƒ­ã‚°å‡ºåŠ›")
    parser.add_argument("--json", action="store_true", help="JSONå½¢å¼ã§å‡ºåŠ›")
    parser.add_argument("--timeout", type=int, default=1800, help="ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 1800ç§’=30åˆ†ï¼‰")
    parser.add_argument("--model", help="AIãƒ¢ãƒ‡ãƒ«ï¼ˆhaiku/sonnet/opusï¼‰")
    parser.add_argument("--auto-review", action="store_true", default=True, help="Workerå®Œäº†å¾Œã«ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’è‡ªå‹•å®Ÿè¡Œï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: æœ‰åŠ¹ï¼‰")
    parser.add_argument("--no-review", action="store_true", help="è‡ªå‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’ç„¡åŠ¹åŒ–ï¼ˆæ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹å ´åˆï¼‰")
    parser.add_argument("--review-model", default="sonnet", help="ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨AIãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: sonnetï¼‰")
    parser.add_argument("--loop", action="store_true", help="ã‚¿ã‚¹ã‚¯å®Œäº†å¾Œã«æ¬¡ã®QUEUEDã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•èµ·å‹•ï¼ˆé€£ç¶šå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ï¼‰")
    parser.add_argument("--max-tasks", type=int, default=100, help="é€£ç¶šå®Ÿè¡Œæ™‚ã®æœ€å¤§ã‚¿ã‚¹ã‚¯æ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ï¼‰")
    parser.add_argument("--is-rework", action="store_true", help="ãƒªãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼ˆå·®ã—æˆ»ã—å¯¾å¿œï¼‰")
    parser.add_argument("--rework-comment", help="å·®ã—æˆ»ã—ã‚³ãƒ¡ãƒ³ãƒˆï¼ˆå•é¡Œç‚¹ãƒ»ä¿®æ­£æŒ‡é‡ï¼‰")
    parser.add_argument("--parallel", action="store_true", help="ä¸¦åˆ—èµ·å‹•ãƒ¢ãƒ¼ãƒ‰ï¼ˆORDERé–‹å§‹æ™‚ã«ä¸¦åˆ—ã‚¿ã‚¹ã‚¯ã‚’è‡ªå‹•æ¤œå‡ºãƒ»èµ·å‹•ï¼‰")
    parser.add_argument("--max-workers", type=int, default=5, help="ä¸¦åˆ—èµ·å‹•æ™‚ã®æœ€å¤§Workeræ•°ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 5ï¼‰")
    parser.add_argument("--allowed-tools", type=str, default=None,
                        help="ã‚«ãƒ³ãƒžåŒºåˆ‡ã‚Šã®è¨±å¯ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆï¼ˆä¾‹: Read,Write,Bashï¼‰ã€‚æœªæŒ‡å®šæ™‚ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ¨©é™ã‚’ä½¿ç”¨")

    args = parser.parse_args()

    # è©³ç´°ãƒ­ã‚°ãƒ¢ãƒ¼ãƒ‰
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)

    # ä¸¦åˆ—èµ·å‹•ãƒ¢ãƒ¼ãƒ‰
    if args.parallel:
        try:
            from worker.parallel_launcher import ParallelWorkerLauncher

            # ã‚¿ã‚¹ã‚¯IDã‹ã‚‰ORDER_IDã‚’å–å¾—
            conn = get_connection()
            try:
                task_row = fetch_one(
                    conn,
                    "SELECT order_id FROM tasks WHERE id = ? AND project_id = ?",
                    (f"TASK_{args.task_id}" if not args.task_id.startswith("TASK_") else args.task_id, args.project_id)
                )

                if not task_row or not task_row["order_id"]:
                    print("ã‚¨ãƒ©ãƒ¼: ã‚¿ã‚¹ã‚¯ã«ç´ä»˜ãORDERãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", file=sys.stderr)
                    sys.exit(1)

                order_id = task_row["order_id"]

            finally:
                conn.close()

            # allowed_tools ãƒ‘ãƒ¼ã‚¹ï¼ˆä¸¦åˆ—èµ·å‹•ãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰
            parallel_allowed_tools = None
            if args.allowed_tools:
                parallel_allowed_tools = [t.strip() for t in args.allowed_tools.split(",") if t.strip()]

            # ä¸¦åˆ—Workerã‚’èµ·å‹•
            launcher = ParallelWorkerLauncher(
                args.project_id,
                order_id,
                max_workers=args.max_workers,
                dry_run=args.dry_run,
                verbose=args.verbose,
                timeout=args.timeout,
                model=args.model,
                no_review=args.no_review,
                allowed_tools=parallel_allowed_tools,
            )

            results = launcher.launch()

            # çµæžœè¡¨ç¤º
            if args.json:
                print(json.dumps(results, ensure_ascii=False, indent=2))
            else:
                from worker.parallel_launcher import display_results
                display_results(results, json_output=False)

            # Exit code
            if results["launched_count"] > 0:
                sys.exit(0)
            else:
                sys.exit(1)

        except ImportError as e:
            print(f"ã‚¨ãƒ©ãƒ¼: ä¸¦åˆ—èµ·å‹•ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆå¤±æ•— - {e}", file=sys.stderr)
            sys.exit(1)
        except Exception as e:
            print(f"ã‚¨ãƒ©ãƒ¼: ä¸¦åˆ—èµ·å‹•å¤±æ•— - {e}", file=sys.stderr)
            import traceback
            traceback.print_exc()
            sys.exit(1)

    # ãƒ«ãƒ¼ãƒ—å®Ÿè¡Œç”¨å¤‰æ•°
    current_task_id = args.task_id
    executed_count = 0
    all_results = []

    while True:
        executed_count += 1

        # æœ€å¤§ã‚¿ã‚¹ã‚¯æ•°ãƒã‚§ãƒƒã‚¯
        if executed_count > args.max_tasks:
            logger.warning(f"æœ€å¤§ã‚¿ã‚¹ã‚¯æ•° ({args.max_tasks}) ã«é”ã—ã¾ã—ãŸã€‚ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break

        # allowed_tools ãƒ‘ãƒ¼ã‚¹
        allowed_tools = None
        if args.allowed_tools:
            allowed_tools = [t.strip() for t in args.allowed_tools.split(",") if t.strip()]

        # Workerå‡¦ç†å®Ÿè¡Œ
        executor = WorkerExecutor(
            args.project_id,
            current_task_id,
            dry_run=args.dry_run,
            skip_ai=args.skip_ai,
            verbose=args.verbose,
            timeout=args.timeout,
            model=args.model,
            auto_review=args.auto_review and not args.no_review,
            review_model=args.review_model,
            loop=args.loop,
            max_tasks=args.max_tasks,
            is_rework=args.is_rework,
            rework_comment=args.rework_comment,
            allowed_tools=allowed_tools,
        )

        results = executor.execute()
        all_results.append(results)

        # å‡ºåŠ›
        if args.json:
            # å¤§ããªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã¯é™¤å¤–
            output = {k: v for k, v in results.items() if k not in ("execution_result",)}
            print(json.dumps(output, ensure_ascii=False, indent=2, default=str))
        else:
            if results["success"]:
                print(f"ã€Workerå‡¦ç†å®Œäº†ã€‘{results['task_id']} ({executed_count}/{args.max_tasks if args.loop else 1})")
                print(f"  ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ: {results['project_id']}")
                print(f"  Worker: {results.get('worker_id', 'Auto')}")
                if results.get("report_file"):
                    print(f"  REPORT: {results['report_file']}")
                # è‡ªå‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæžœã®è¡¨ç¤º
                review_result = results.get("review_result")
                if review_result:
                    if review_result.get("success"):
                        verdict = review_result.get("verdict", "UNKNOWN")
                        print(f"  ã€è‡ªå‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€‘{verdict}")
                        if review_result.get("review_file"):
                            print(f"  REVIEW: {review_result['review_file']}")
                    else:
                        print(f"  ã€è‡ªå‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ã€‘å¤±æ•—: {review_result.get('error', 'ä¸æ˜Ž')}")
            else:
                print(f"ã€Workerå‡¦ç†å¤±æ•—ã€‘{results.get('error', 'ä¸æ˜Žãªã‚¨ãƒ©ãƒ¼')}", file=sys.stderr)
                sys.exit(1)

        # ãƒ«ãƒ¼ãƒ—ãƒ¢ãƒ¼ãƒ‰ã§ãªã„å ´åˆã€ã¾ãŸã¯æ¬¡ã‚¿ã‚¹ã‚¯ãŒãªã„å ´åˆã¯çµ‚äº†
        if not args.loop:
            break

        next_task = results.get("next_task")
        if not next_task:
            print("\nã€é€£ç¶šå®Ÿè¡Œå®Œäº†ã€‘QUEUEDã‚¿ã‚¹ã‚¯ãŒãªããªã‚Šã¾ã—ãŸã€‚")
            print(f"  å®Ÿè¡Œã‚¿ã‚¹ã‚¯æ•°: {executed_count}")
            break

        # æ¬¡ã‚¿ã‚¹ã‚¯ã¸
        print(f"\nã€æ¬¡ã‚¿ã‚¹ã‚¯èµ·å‹•ã€‘{next_task}")
        current_task_id = next_task


if __name__ == "__main__":
    main()
